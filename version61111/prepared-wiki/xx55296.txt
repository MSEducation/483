         an integer (the [[Array data type|zero-based]] position in S at which W is found)
     '''define variables''':
         an integer, m ← 0 (the beginning of the current match in S)
         an integer, i ← 0 (the position of the current character in W)
         an array of integers, T (the table, computed elsewhere)
     '''while''' m + i < length(S) '''do'''
         '''if''' W[i] = S[m + i] '''then'''
             '''if''' i = length(W) - 1 '''then'''
                 '''return''' m
             '''let''' i ← i + 1
         '''else'''
             '''if''' T[i] > -1 '''then'''
                 '''let''' i ← T[i], m ← m + i - T[i]
             '''else'''
                 '''let''' i ← 0, m ← m + 1
     (if we reach here, we have searched all of S unsuccessfully)
     '''return''' the length of S

===Efficiency of the search algorithm===

Assuming the prior existence of the table T, the search portion of the Knuth–Morris–Pratt algorithm has complexity O(n), where n is the length of S and the O is big-O notation.  Except for the fixed overhead incurred in entering and exiting the function, all the computations are performed in the while loop. To bound the number of iterations of this loop; observe that T is constructed so that if a match which had begun at Sm fails while comparing S+ i to Wi, then the next possible match must begin at S+ (i - Ti).  In particular the next possible match must occur at a higher index than m, so that Ti .
This fact implies that the loop can execute at most 2n times.  For, in each iteration, it executes one of the two branches in the loop.  The first branch invariably increases i and does not change m, so that the index m + i of the currently scrutinized character of S is increased.  The second branch adds i - Ti to m, and as we have seen, this is always a positive number.  Thus the location m of the beginning of the current potential match is increased.  Now, the loop ends if m + i = n; therefore each branch of the loop can be reached at most k times, since they respectively increase either m + i or m, and m ≤ m + i: if m = n, then certainly m + i ≥ n, so that since it increases by unit increments at most, we must have had m + i = n at some point in the past, and therefore either way we would be done.
Thus the loop executes at most 2n times, showing that the time complexity of the search algorithm is O(n).
Here is another way to think about the runtime:
Let us say we begin to match W and S at position i and p, if W exists as a substring of S at p, then Wthrough m == Sthrough p+m.
Upon success, that is, the word and the text matched at the positions(Wi == Sp+i), we increase i by 1 (i++).
Upon failure, that is, the word and the text does not match at the positions(Wi != Sp+i), the text pointer is kept still, while the word pointer roll-back a certain amount(i = Ti, where T is the jump table) And we attempt to match WTi with Sp+i.
The maximum number of roll-back of i is bounded by i, that is to say, for any failure, we can only roll-back as much as we have progressed up to the failure.
Then it is clear the runtime is 2n.

=="Partial match" table (also known as "failure function")==

The goal of the table is to allow the algorithm not to match any character of S more than once.  The key observation about the nature of a linear search that allows this to happen is that in having checked some segment of the main string against an initial segment of the pattern, we know exactly at which places a new potential match which could continue to the current position could begin prior to the current position.  In other words, we "pre-search" the pattern itself and compile a list of all possible fallback positions that bypass a maximum of hopeless characters while not sacrificing any potential matches in doing so.
We want to be able to look up, for each position in W, the length of the longest possible initial segment of W leading up to (but not including) that position, other than the full segment starting at W0 that just failed to match; this is how far we have to backtrack in finding the next match.  Hence Ti is exactly the length of the longest possible proper initial segment of W which is also a segment of the substring ending at W- 1.  We use the convention that the empty string has length 0.  Since a mismatch at the very start of the pattern is a special case (there is no possibility of backtracking), we set T0 = -1, as discussed below.

===Worked example of the table-building algorithm===

We consider the example of W = "ABCDABD" first.  We will see that it follows much the same pattern as the main search, and is efficient for similar reasons.  We set T0 = -1.  To find T1, we must discover a proper suffix of "A" which is also a prefix of W.  But there are no proper suffixes of "A", so we set T1 = 0.  Likewise, T2 = 0.
Continuing to T3, we note that there is a shortcut to checking all suffixes: let us say that we discovered a proper suffix which is a proper prefix and ending at W2 with length 2 (the maximum possible); then its first character is a proper prefix of W, hence a proper prefix itself, and it ends at W1, which we already determined cannot occur in case T2. Hence at each stage, the shortcut rule is that one needs to consider checking suffixes of a given size m+1 only if a valid suffix of size m was found at the previous stage (e.g. Tx=m).
Therefore we need not even concern ourselves with substrings having length 2, and as in the previous case the sole one with length 1 fails, so T3 = 0.
We pass to the subsequent W4, 'A'.  The same logic shows that the longest substring we need consider has length 1, and although in this case 'A' does work, recall that we are looking for segments ending before the current character; hence T4 = 0 as well.
Considering now the next character, W5, which is 'B', we exercise the following logic: if we were to find a subpattern beginning before the previous character W4, yet continuing to the current one W5, then in particular it would itself have a proper initial segment ending at W4 yet beginning before it, which contradicts the fact that we already found that 'A' itself is the earliest occurrence of a proper segment ending at W4.  Therefore we need not look before W4 to find a terminal string for W5. Therefore T5 = 1.
Finally, we see that the next character in the ongoing segment starting at W4 = 'A' would be 'B', and indeed this is also W5.  Furthermore, the same argument as above shows that we need not look before W4 to find a segment for W6, so that this is it, and we take T6 = 2.
Therefore we compile the following table:
Other example more interesting and complex:

===Description of pseudocode for the table-building algorithm===

The example above illustrates the general technique for assembling the table with a minimum of fuss.  The principle is that of the overall search: most of the work was already done in getting to the current position, so very little needs to be done in leaving it.  The only minor complication is that the logic which is correct late in the string erroneously gives non-proper substrings at the beginning.  This necessitates some initialization code.
 '''algorithm''' ''kmp_table'':
     '''input''':
         an array of characters, W (the word to be analyzed)
         an array of integers, T (the table to be filled)
     '''output''':
         nothing (but during operation, it populates the table)
     '''define variables''':
         an integer, pos ← 2 (the current position we are computing in T)
         an integer, cnd ← 0 (the zero-based index in W of the next <br>character of the current candidate substring)
     (the first few values are fixed but different from what the algorithm <br>might suggest)
     '''let''' T[0] ← -1, T[1] ← 0
     '''while''' pos < length(W) do
         (first case: the substring continues)
         '''if''' W[pos - 1] = W[cnd] '''then'''
             '''let''' cnd ← cnd + 1, T[pos] ← cnd, pos ← pos + 1
         (second case: it doesn't, but we can fall back)
         '''else''' '''if''' cnd > 0 '''then'''
             '''let''' cnd ← T[cnd]
         (third case: we have run out of candidates.  Note cnd = 0)
         '''else'''
             '''let''' T[pos] ← 0, pos ← pos + 1

===Efficiency of the table-building algorithm===

The complexity of the table algorithm is O(n), where n is the length of W.  As except for some initialization all the work is done in the while loop, it is sufficient to show that this loop executes in O(n) time, which will be done by simultaneously examining the quantities pos and pos - cnd.  In the first branch, pos - cnd is preserved, as both pos and cnd are incremented simultaneously, but naturally, pos is increased.  In the second branch, cnd is replaced by Tcnd, which we saw above is always strictly less than cnd, thus increasing pos - cnd.  In the third branch, pos is incremented and cnd is not, so both pos and pos - cnd increase. Since pos ≥ pos - cnd, this means that at each stage either pos or a lower bound for pos increases; therefore since the algorithm terminates once pos = n, it must terminate after at most 2n iterations of the loop, since pos - cnd begins at 1.  Therefore the complexity of the table algorithm is O(n).

==Efficiency of the KMP algorithm==

Since the two portions of the algorithm have, respectively, complexities of O(k) and O(n), the complexity of the overall algorithm is O(n + k).
These complexities are the same, no matter how many repetitive patterns are in W or S.

==Variants==

The Booth algorithm uses a modified version of the KMP preprocessing function to find the lexicographically minimal string rotation. The failure function is progressively calculated as the string is rotated.

==See also==

==References==

==External links==


