</ref> If the underlying generating models are linear then a minimum-variance [[Kalman filter]] and a minimum-variance smoother may be used to recover data of interest from noisy measurements. The afore-mentioned techniques rely on one-step-ahead predictors (which minimise the variance of the prediction error). When the generating models are nonlinear then step-wise linearizations may be applied within [[Extended Kalman Filter]] and smoother recursions. However, in nonlinear cases, optimum minimum-variance performance guarantees no longer apply.
Statistical techniques used for prediction include regression analysis and time series analysis, and their various sub-categories such as ordinary least squares, logistic regression, autoregressive moving average models,  and vector autoregression models.
To use regression analysis for prediction, data are collected on the variable that is to be predicted, called the dependent variable or response variable, and on one or more variables whose values are hypothesized to influence it, called independent variables or explanatory variables. A functional form, often linear, is hypothesized for the postulated causal relationship, and the parameters of the function are estimated from the data—that is, are chosen so as to optimize is some way the fit of the function, thus parametrized, to the data. That much is the estimation step. For the prediction step, explanatory variable values that are deemed relevant to future (or current but not yet observed) values of the dependent variable are used in the parametrized function to generate predictions for the dependent variable.
An issue before finding mathematical relationship is to find what series of data to compare. Here big data multiply the areas explored and improve the possibility to find correlations between phenomena where no connection was suspected before.

==Prediction in science==

In science, a prediction is a rigorous, often quantitative, statement, forecasting what will happen under specific conditions; for example, if an apple falls from a tree it will be attracted towards the center of the earth by gravity with a specified and constant acceleration. The scientific method is built on testing statements that are logical consequences of scientific theories. This is done through repeatable experiments or observational studies.
A scientific theory whose statements are contradicted by observations and evidence will be rejected. New theories that generate many new predictions can more easily be supported or falsified (see predictive power). Notions that make no testable predictions are usually considered not to be part of science (protoscience or nescience) until testable predictions can be made.
Mathematical equations and models, and computer models, are frequently used to describe the past and future behaviour of a process within the boundaries of that model. In some cases the probability of an outcome, rather than a specific outcome, can be predicted, for example in much of quantum physics.
In microprocessors, branch prediction permits avoidance of pipeline emptying at branch instructions. In engineering, possible failure modes are predicted and avoided by correcting the mechanism causing the failure. 
Accurate prediction and forecasting are very difficult in some areas, such as natural disasters, pandemics, demography, population dynamics and meteorology. For example, it is possible to predict the occurrence of solar cycles, but their exact timing and magnitude is much more difficult (see picture to right).

===Scientific hypothesis and prediction===

Established science makes useful predictions which are extremely reliable and accurate; for example, eclipses are routinely predicted. 
New theories make predictions which allow them to be falsified if the predictions are not borne out in reality. For example, in the early twentieth century the scientific consensus was that there existed an absolute frame of reference, given the name luminiferous ether. The existence of this absolute frame was deemed necessary for consistency with the established idea that the speed of light is constant. The famous Michelson-Morley experiment demonstrated that predictions deduced from this concept were not borne out in reality, falsifying the idea of an absolute frame of reference. The special theory of relativity was proposed by Einstein as an explanation for the seeming inconsistency between the constancy of the speed of light and the non-existence of a special, preferred or absolute frame of reference. 
Albert Einstein's theory of general relativity could not easily be tested as it did not produce any effects observable on a terrestrial scale. However, the theory predicted that large masses such as stars would bend light, in contradiction to accepted theory; this was observed in a 1919 eclipse.

==Finance==

Mathematical models of stock market behaviour are also unreliable in predicting future behaviour. Consequently, stock investors may anticipate or predict a stock market boom, or fail to anticipate or predict a stock market crash.
Some correlation has been seen between actual stock market movements and prediction data from large groups in surveys and prediction games.
An actuary uses actuarial science to assess and predict future business risk, such that the risk(s) can be mitigated. For example, in insurance an actuary would use a life table to predict (truly, estimate or compute) life expectancy.

==Sports==

Predicting the outcome of sporting events is a business which has grown in popularity in recent years.  Handicappers predict the outcome of games using a variety of mathematical formulas, simulation models or qualitative analysis.  Early, well known sports bettors, such as Jimmy the Greek, were believed to have access to information that gave them an edge.  Information ranged from personal issues, such as gambling or drinking to undisclosed injuries; anything that may affect the performance of a player on the field.
Recent times have changed the way sports are predicted.  Predictions now typically consist of two distinct approaches: Situational plays and statistical based models.  Situational plays are much more difficult to measure because they usually involve the motivation of a team.  Dan Gordon, noted handicapper, wrote “Without an emotional edge in a game in addition to value in a line, I won’t put my money on it”.[ref]
</ref>  These types of plays consist of: Betting on the home underdog, betting against Monday Night winners if they are a favorite next week, betting the underdog in “look ahead” games etc.  As situational plays become more widely known they become less useful because they will impact the way the line is set.
The widespread use of technology has brought with it more modern sports betting systems.  These systems are typically algorithms and simulation models based on regression analysis.  Jeff Sagarin, a sports statistician, has brought attention to sports by having the results of his models published in USA Today.  He is currently paid as a consultant by the Dallas Mavericks for his advice on lineups and the use of his Winval system, which evaluates free agents.  Brian Burke, a former Navy fighter pilot turned sports statistician, has published his results of using regression analysis to predict the outcome of NFL games.[ref]
