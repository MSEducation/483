</ref> [[gene expression programming]],<ref>[tpl]cite web|last=Ferreira|first=C.|year=2006|title=Designing Neural Networks Using Gene Expression Programming|url= http://www.gene-expression-programming.com/webpapers/Ferreira-ASCT2006.pdf|publisher= In A. Abraham, B. de Baets, M. Köppen, and B. Nickolay, eds., Applied Soft Computing Technologies: The Challenge of Complexity, pages 517–536, Springer-Verlag[/tpl]</ref> [[simulated annealing]],<ref>[tpl]cite conference| author = Da, Y., Xiurun, G. |date=July 2005 | title = An improved PSO-based ANN with simulated annealing technique | conference = New Aspects in Neurocomputing: 11th European Symposium on Artificial Neural Networks | conferenceurl = http://www.dice.ucl.ac.be/esann/proceedings/electronicproceedings.htm | editor = T. Villmann | publisher = Elsevier | accessdate = 30 December 2011 | doi = 10.1016/j.neucom.2004.07.002 [/tpl]</ref> [[expectation-maximization]], [[non-parametric methods]] and [[particle swarm optimization]]<ref>[tpl]cite conference| author = Wu, J., Chen, E. |date=May 2009 | title = A Novel Nonparametric Regression Ensemble for Rainfall Forecasting Using Particle Swarm Optimization Technique Coupled with Artificial Neural Network | conference = 6th International Symposium on Neural Networks, ISNN 2009 | conferenceurl = http://www2.mae.cuhk.edu.hk/~isnn2009/ | editor = Wang, H., Shen, Y., Huang, T., Zeng, Z. | publisher = Springer | accessdate = 1 January 2012 | doi = 10.1007/978-3-642-01513-7_6 | isbn = 978-3-642-01215-0 [/tpl]</ref> are some commonly used methods for training neural networks. [tpl]See also|machine learning[/tpl]

==Employing artificial neural networks==

Perhaps the greatest advantage of ANNs is their ability to be used as an arbitrary function approximation mechanism that 'learns' from observed data. However, using them is not so straightforward, and a relatively good understanding of the underlying theory is essential.
With the correct implementation, ANNs can be used naturally in online learning and large data set applications. Their simple implementation and the existence of mostly local dependencies exhibited in the structure allows for fast, parallel implementations in hardware.

==Applications==

The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations. This is particularly useful in applications where the complexity of the data or task makes the design of such a function by hand impractical.

===Real-life applications===

The tasks artificial neural networks are applied to tend to fall within the following broad categories:
Application areas include the system identification and control (vehicle control, process control, natural resources management), quantum chemistry,[tpl]Cite journal|journal=J. Chem. Phys. |volume = 131 |issue = 7 |page = 074104 |doi=10.1063/1.3206326 |title=Neural network approach to quantum-chemistry data: Accurate prediction of density functional theory energies |year=2009 |author=Roman M. Balabin,  Ekaterina I. Lomakina |pmid=19708729[/tpl] game-playing and decision making (backgammon, chess, poker), pattern recognition (radar systems, face identification, object recognition and more), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications (e.g. automated trading systems), data mining (or knowledge discovery in databases, "KDD"), visualization and e-mail spam filtering.
Artificial neural networks have also been used to diagnose several cancers.  An ANN based hybrid lung cancer detection system named HLND improves the accuracy of diagnosis and the speed of lung cancer radiology.[tpl]cite web|last=Ganesan|first=N|title=Application of Neural Networks in Diagnosing Cancer Disease Using Demographic Data|url=http://www.ijcaonline.org/journal/number26/pxc387783.pdf|publisher=International Journal of Computer Applications[/tpl]  These networks have also been used to diagnose prostate cancer.  The diagnoses can be used to make specific models taken from a large group of patients compared to information of one given patient.  The models do not depend on assumptions about correlations of different variables.  Colorectal cancer has also been predicted using the neural networks.  Neural networks could predict the outcome for a patient with colorectal cancer with more accuracy than the current clinical methods.  After training, the networks could predict multiple patient outcomes from unrelated institutions.[tpl]cite web|last=Bottaci|first=Leonardo|title=Artificial Neural Networks Applied to Outcome Prediction for Colorectal Cancer Patients in Separate Institutions|url=http://www.lcc.uma.es/~jja/recidiva/042.pdf|publisher=The Lancet[/tpl]

===Neural networks and neuroscience===

Theoretical and computational neuroscience is the field concerned with the theoretical analysis and the computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behavior, the field is closely related to cognitive and behavioral modeling.
The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).

====Types of models====

Many models are used in the field, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.

==Neural network software==

Neural network software is used to simulate, research, develop and apply artificial neural networks, biological neural networks and, in some cases, a wider array of adaptive systems.

==Types of artificial neural networks==

Artificial neural network types vary from those with only one or two layers of single direction logic, to complicated multi–input many directional feedback loops and layers. On the whole, these systems use algorithms in their programming to determine control and organization of their functions. Some may be as simple as a one-neuron layer with an input and an output, and others can mimic complex systems such as dANN, which can mimic chromosomal DNA through sizes at the cellular level, into artificial organisms and simulate reproduction, mutation and population sizes.[tpl]cite web|title=DANN:Genetic Wavelets|url=http://wiki.syncleus.com/index.php/DANN:Genetic_Wavelets|publisher=dANN project|accessdate=12 July 2010| archiveurl= http://web.archive.org/web/20100821112612/http://wiki.syncleus.com/index.php/DANN:Genetic_Wavelets| archivedate= 21 August 2010 | deadurl= no[/tpl]
Most systems use "weights" to change the parameters of the throughput and the varying connections to the neurons. Artificial neural networks can be autonomous and learn by input from outside "teachers" or even self-teaching from written-in rules.

==Theoretical properties==

===Computational power===

The multi-layer perceptron (MLP) is a universal function approximator, as proven by the Cybenko theorem. However, the proof is not constructive regarding the number of neurons required or the settings of the weights.
Work by Hava Siegelmann and Eduardo D. Sontag has provided a proof that a specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a Universal Turing Machine[tpl]Cite journal| title =  Turing computability with neural nets | url = http://www.math.rutgers.edu/~sontag/FTP_DIR/aml-turing.pdf | year = 1991 | journal = Appl. Math. Lett. | pages = 77–80 | volume = 4 | issue = 6 | last1 = Siegelmann | first1 =  H.T. | last2 =  Sontag | first2 =  E.D. | doi =  10.1016/0893-9659(91)90080-F [/tpl] using a finite number of neurons and standard linear connections. They have further shown that the use of irrational values for weights results in a machine with super-Turing power.[tpl]Citation needed|date=August 2011[/tpl]

===Capacity===

Artificial neural network models have a property called 'capacity', which roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.

===Convergence===

Nothing can be said in general about convergence since it depends on a number of factors. Firstly, there may exist many local minima. This depends on the cost function and the model. Secondly, the optimization method used might not be guaranteed to converge when far away from a local minimum. Thirdly, for a very large amount of data or parameters, some methods become impractical. In general, it has been found that theoretical guarantees regarding convergence are an unreliable guide to practical application. [tpl]Citation needed|date=March 2012[/tpl]

===Generalization and statistics===

In applications where the goal is to create a system that generalizes well in unseen examples, the problem of over-training has emerged. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. There are two schools of thought for avoiding this problem: The first is to use cross-validation and similar techniques to check for the presence of overtraining and optimally select hyperparameters such as to minimize the generalization error. The second is to use some form of regularization. This is a concept that emerges naturally in a probabilistic (Bayesian) framework, where the regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.
Supervised neural networks that use an MSE cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.
By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.
The softmax activation function is:

===Dynamic properties===

Various techniques originally developed for studying disordered magnetic systems (i.e., the spin glass) have been successfully applied to simple neural network architectures, such as the Hopfield network. Influential work by E. Gardner and B. Derrida has revealed many interesting properties about perceptrons with real-valued synaptic weights, while later work by W. Krauth and M. Mezard has extended these principles to binary-valued synapses.

==Criticism==

A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper "Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving," uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns – it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.
A. K. Dewdney, a former Scientific American columnist, wrote in 1997, "Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool." (Dewdney, p. 82)
Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections – which can consume vast amounts of computer memory and hard disk space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons – which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield effective programs, they too often do so at the cost of efficiency (they tend to consume considerable amounts of time and money).
Arguments against Dewdney's position are that neural nets have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraftNASA - Dryden Flight Research Center - News Room: News Releases: NASA NEURAL NETWORK PROJECT PASSES MILESTONE. Nasa.gov. Retrieved on 2013-11-20. to detecting credit card fraud .[tpl]citation needed|date=August 2012[/tpl]
Technology writer Roger Bridgman commented on Dewdney's statements about neural nets:
 Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".
 In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.Roger Bridgman's defence of neural networks

In response to this kind of criticism, one should note that although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/4
Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).

==Gallery==

Image:Two_layer_ann.svg|A two-layer feedforward artificial neural network.
Image:Artificial_neural_network.svg
Image:Ann_dependency_(graph).svg

==See also==

==References==

==Bibliography==

 
 title=Neural Networks in Materials Science | journal=ISIJ International | volume=39 |pages=966–979 | doi=10.2355/isijinternational.39.966 | url=http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.pdf| issue=10}}
title=Image processing with neural networks – a review | journal=Pattern Recognition | volume=35 | pages=2279–2301 | doi = 10.1016/S0031-3203(01)00178-9 | issue=10
}}

==External links==


