[[Nyquist–Shannon sampling theorem]]

CATEGORIES: Digital signal processing, Information theory, Theorems in Fourier analysis, Articles containing proofs, Mathematical theorems in theoretical computer science

In the field of digital signal processing, the sampling theorem is a fundamental bridge between continuous signals (analog domain) and discrete signals (digital domain).  Strictly speaking, it only applies to a class of mathematical functions whose Fourier transforms are zero outside of a finite region of frequencies (see Fig 1).  The analytical extension to actual signals, which can only approximate that condition, is provided by the discrete-time Fourier transform, a version of the Poisson summation formula.   Intuitively we expect that when one reduces a continuous function to a discrete sequence (called samples) and interpolates back to a continuous function, the fidelity of the result depends on the density (or sample-rate) of the original samples.  The sampling theorem introduces the concept of a sample-rate that is sufficient for perfect fidelity for the class of bandlimited functions.  And it expresses the sample-rate in terms of the function's bandwidth.  Thus no actual "information" is lost during the sampling process.  The theorem also leads to a formula for the mathematically ideal interpolation algorithm.
The theorem does not preclude the possibility of perfect reconstruction under special circumstances that do not satisfy the sample-rate criterion.  (See Sampling of non-baseband signals below, and Compressed sensing.)
The name Nyquist–Shannon sampling theorem honors Harry Nyquist and Claude Shannon.  The theorem was also discovered independently by E. T. Whittaker, by Vladimir Kotelnikov, and by others.  So it is also known by the names Nyquist–Shannon–Kotelnikov, Whittaker–Shannon–Kotelnikov, Whittaker–Nyquist–Kotelnikov–Shannon, and cardinal theorem of interpolation. 

==Introduction==

Sampling is the process of converting a signal (for example, a function of continuous time or space) into a numeric sequence (a function of discrete time or space). Shannon's version of the theorem states:C. E. Shannon, "Communication in the presence of noise", Proc. Institute of Radio Engineers, vol. 37, no. 1, pp. 10–21, Jan. 1949. Reprint as classic paper in: Proc. IEEE, vol. 86, no. 2, (Feb. 1998)
 If a function x(t) contains no frequencies higher than B hertz, it is completely determined by giving its ordinates at a series of points spaced 1/(2B) seconds apart. 
Practical digital-to-analog converters produce neither scaled and delayed sinc functions, nor ideal Dirac pulses.  Instead they produce a piecewise-constant sequence of scaled and delayed rectangular pulses, usually followed by a "shaping filter" to clean up spurious high-frequency content.

==Aliasing==

Let X(f) be the Fourier transform of bandlimited  function x(t):
The Poisson summation formula shows that the samples, x(nT), of function x(t) are sufficient to create a periodic summation of function X(f). The result is:
which is a periodic function and its equivalent representation as a Fourier series, whose coefficients are xn.  This function is also known as the discrete-time Fourier transform (DTFT).  As depicted in Figures 4 and 5, copies of X(f) are shifted by multiples of fs and combined by addition.
If the Nyquist criterion is not satisfied, adjacent copies overlap, and it is not possible in general to discern an unambiguous X(f).  Any frequency component above fs/2 is indistinguishable from a lower-frequency component, called an alias, associated with one of the copies.  In such cases, the customary interpolation techniques produce the alias, rather than the original component.  When the sample-rate is pre-determined by other considerations (such as an industry standard), x(t) is usually filtered to reduce its high frequencies to acceptable levels before it is sampled.  The type of filter required is a lowpass filter, and in this application it is called an anti-aliasing filter.

==Derivation as a special case of Poisson summation==

From Figure 5, it is apparent that when there is no overlap of the copies (aka "images") of X(f), the k = 0 term of Xs(f) can be recovered by the product:
At this point, the sampling theorem is proved, since X(f) uniquely determines x(t).  
All that remains is to derive the formula for reconstruction.  H(f) need not be precisely defined in the region [tpl]nowrap|fs − B[/tpl] because Xs(f) is zero in that region.  However, the worst case is when B = fs/2, the Nyquist frequency. A function that is sufficient for that and all less severe cases is:
where rect(•) is the rectangular function.  Therefore:
The inverse transform of both sides produces the Whittaker–Shannon interpolation formula:
which shows how the samples, x(nT), can be combined to reconstruct x(t).

==Shannon's original proof==

Poisson shows that the Fourier series in [tpl]EquationNote|Eq.1[/tpl] produces the periodic summation of X(f), regardless of fs and B.  Shannon, however, only derives the series coefficients for the case fs = 2B.  Quoting Shannon's original paper, which uses f for the function, F for the spectrum, and W instead of B:
|-
|-
|
|}
Shannon's proof of the theorem is complete at that point, but he goes on to discuss reconstruction via sinc functions, what we now call the Whittaker–Shannon interpolation formula as discussed above.  He does not derive or prove the properties of the sinc function, but these would have been familiar to engineers reading his works at the time, since the Fourier pair relationship between rect (the rectangular function) and sinc was well known.  Quoting Shannon:
As in the other proof, the existence of the Fourier transform of the original signal is assumed, so the proof does not say whether the sampling theorem extends to bandlimited stationary random processes.

===Notes===

==Application to multivariable signals and images==

The sampling theorem is usually formulated for functions of a single variable.  Consequently, the theorem is directly applicable to time-dependent signals and is normally formulated in that context.  However, the sampling theorem can be extended in a straightforward way to functions of arbitrarily many variables.  Grayscale images, for example, are often represented as two-dimensional arrays (or matrices) of real numbers representing the relative intensities of pixels (picture elements) located at the intersections of row and column sample locations.  As a result, images require two independent variables, or indices, to specify each pixel uniquely — one for the row, and one for the column.
Color images typically consist of a composite of three separate grayscale images, one to represent each of the three primary colors — red, green, and blue, or RGB for short.  Other colorspaces using 3-vectors for colors include HSV, CIELAB, XYZ, etc.  Some colorspaces such as cyan, magenta, yellow, and black (CMYK) may represent color by four dimensions.  All of these are treated as vector-valued functions over a two-dimensional sampled domain.
Similar to one-dimensional discrete-time signals, images can also suffer from aliasing if the sampling resolution, or pixel density, is inadequate.  For example, a digital photograph of a striped shirt with high frequencies (in other words, the distance between the stripes is small), can cause aliasing of the shirt when it is sampled by the camera's image sensor.  The aliasing appears as a moiré pattern.  The "solution" to higher sampling in the spatial domain for this case would be to move closer to the shirt, use a higher resolution sensor, or to optically blur the image before acquiring it with the sensor.
Another example is shown to the right in the brick patterns. The top image shows the effects when the sampling theorem's condition is not satisfied. When software rescales an image (the same process that creates the thumbnail shown in the lower image) it, in effect, runs the image through a low-pass filter first and then downsamples the image to result in a smaller image that does not exhibit the moiré pattern. The top image is what happens when the image is downsampled without low-pass filtering: aliasing results.
The application of the sampling theorem to images should be made with care.  For example, the sampling process in any standard image sensor (CCD or CMOS camera) is relatively far from the ideal sampling which would measure the image intensity at a single point.  Instead these devices have a relatively large sensor area at each sample point in order to obtain sufficient amount of light. In other words, any detector has a finite-width point spread function. The analog optical image intensity function which is sampled by the sensor device is not in general bandlimited, and the non-ideal sampling is itself a useful type of low-pass filter, though not always sufficient to remove enough high frequencies to sufficiently reduce aliasing.  When the area of the sampling spot (the size of the pixel sensor) is not large enough to provide sufficient spatial anti-aliasing, a separate anti-aliasing filter (optical low-pass filter) is typically included in a camera system to further blur the optical image.  Despite images having these problems in relation to the sampling theorem, the theorem can be used to describe the basics of down and up sampling of images.

==Critical frequency==

To illustrate the necessity of fs > 2B, consider the family of sinusoids (depicted in Fig. 8) generated by different values of θ in this formula:
With fs = 2B or equivalently T = 1/(2B), the samples are given by:
regardless of the value of θ.  That sort of ambiguity is the reason for the strict inequality of the sampling theorem's condition.

==Sampling of non-baseband signals==

As discussed by Shannon:
That is, a sufficient no-loss condition for sampling signals that do not have baseband components exists that involves the width of the non-zero frequency interval as opposed to its highest frequency component. See Sampling (signal processing) for more details and examples.
A bandpass condition is that X(f) = 0, for all nonnegative f outside the open band of frequencies:
for some nonnegative integer N.  This formulation includes the normal baseband condition as the case N=0.
The corresponding interpolation function is the impulse response of an ideal brick-wall bandpass filter (as opposed to the ideal brick-wall lowpass filter used above) with cutoffs at the upper and lower edges of the specified band, which is the difference between a pair of lowpass impulse responses:
Other generalizations, for example to signals occupying multiple non-contiguous bands, are possible as well.  Even the most generalized form of the sampling theorem does not have a provably true converse.  That is, one cannot conclude that information is necessarily lost just because the conditions of the sampling theorem are not satisfied; from an engineering perspective, however, it is generally safe to assume that if the sampling theorem is not satisfied then information will most likely be lost.

==Nonuniform sampling==

The sampling theory of Shannon can be generalized for the case of nonuniform sampling, that is, samples not taken equally spaced in time. The Shannon sampling theory for non-uniform sampling states that a band-limited signal can be perfectly reconstructed from its samples if the average sampling rate satisfies the Nyquist condition.Nonuniform Sampling, Theory and Practice (ed. F. Marvasti), Kluwer Academic/Plenum Publishers, New York, 2000 Therefore, although uniformly spaced samples may result in easier reconstruction algorithms, it is not a necessary condition for perfect reconstruction.
The general theory for non-baseband and nonuniform samples was developed in 1967 by Landau.[tpl]cite journal |first=H. J. |last=Landau |title=Necessary density conditions for sampling and interpolation of certain entire functions |journal=Acta Math. |volume=117 |issue=1 |pages=37–52 |year=1967 |doi=10.1007/BF02395039 [/tpl]  He proved that, to paraphrase roughly, the average sampling rate (uniform or otherwise) must be twice the occupied bandwidth of the signal, assuming it is a priori known what portion of the spectrum was occupied.
In the late 1990s, this work was partially extended to cover signals of when the amount of occupied bandwidth was known, but the actual occupied portion of the spectrum was unknown.see, e.g., [tpl]cite book |first=P. |last=Feng |title=Universal minimum-rate sampling and spectrum-blind reconstruction for multiband signals |publisher=Ph.D. dissertation, University of Illinois at Urbana-Champaign |year=1997 [/tpl]  In the 2000s, a complete theory was developed
(see the section Beyond Nyquist below) using compressed sensing.  In particular, the theory, using signal processing language, is described in this 2009 paper.[tpl]cite journal | id = [tpl]citeseerx|10.1.1.154.4255[/tpl] | title = Blind Multiband Signal Reconstruction: Compressed Sensing for Analog Signals | first1 = Moshe | last1 = Mishali | first2 = Yonina C. | last2 = Eldar | journal = IEEE Trans. Signal Processing | month = March | year = 2009 | volume = 57 | issue = 3 [/tpl]  They show, among other things, that if the frequency locations are unknown, then it is necessary to sample at least at twice the Nyquist criteria; in other words, you must pay at least a factor of 2 for not knowing the location of the spectrum.  Note that minimum sampling requirements do not necessarily guarantee stability.

==Sampling below the Nyquist rate under additional restrictions==

The Nyquist–Shannon sampling theorem provides a sufficient condition for the sampling and reconstruction of a band-limited signal.  When reconstruction is done via the Whittaker–Shannon interpolation formula, the Nyquist criterion is also a necessary condition to avoid aliasing, in the sense that if samples are taken at a slower rate than twice the band limit, then there are some signals that will not be correctly reconstructed.  However, if further restrictions are imposed on the signal, then the Nyquist criterion may no longer be a necessary condition.
A non-trivial example of exploiting extra assumptions about the signal is given by the recent field of compressed sensing, which allows for full reconstruction with a sub-Nyquist sampling rate. Specifically, this applies to signals that are sparse (or compressible) in some domain.  As an example, compressed sensing deals with signals that may have a low over-all bandwidth (say, the effective bandwidth EB),  but the frequency locations are unknown, rather than all together in a single band, so that the passband technique doesn't apply.  In other words, the frequency spectrum is sparse.  Traditionally, the necessary sampling rate is thus 2B.  Using compressed sensing techniques, the signal could be perfectly reconstructed if it is sampled at a rate slightly lower than 2EB.   The downside of this approach is that reconstruction is no longer given by a formula, but instead by the solution to a convex optimization program which requires well-studied but nonlinear methods.

==Historical background==

The sampling theorem was implied by the work of Harry Nyquist in 1928 ("Certain topics in telegraph transmission theory"), in which he showed that up to 2B independent pulse samples could be sent through a system of bandwidth B; but he did not explicitly consider the problem of sampling and reconstruction of continuous signals.  About the same time, Karl Küpfmüller showed a similar result,[tpl]cite journal |first=Karl |last=Küpfmüller |title=Über die Dynamik der selbsttätigen Verstärkungsregler |journal=Elektrische Nachrichtentechnik |volume=5 |issue=11 |pages=459–467 |year=1928 [/tpl] [tpl]de icon[/tpl] (English translation 2005). and discussed the sinc-function impulse response of a band-limiting filter, via its integral, the step response Integralsinus; this bandlimiting and reconstruction filter that is so central to the sampling theorem is sometimes referred to as a Küpfmüller filter (but seldom so in English).
The sampling theorem, essentially a dual of Nyquist's result, was proved by Claude E. Shannon in 1949 ("Communication in the presence of noise").
V. A. Kotelnikov published similar results in 1933 ("On the transmission capacity of the 'ether' and of cables in electrical communications", translation from the Russian), as did the mathematician E. T. Whittaker in 1915  ("Expansions of the Interpolation-Theory", "Theorie der Kardinalfunktionen"), J. M. Whittaker in 1935 ("Interpolatory function theory"), and Gabor in 1946 ("Theory of communication").

===Other discoverers===

Others who have independently discovered or played roles in the development of the sampling theorem have been discussed in several historical articles, for example by JerriAbdul Jerri, The Shannon Sampling Theorem—Its Various Extensions and Applications: A Tutorial Review, Proceedings of the IEEE, 65:1565–1595, Nov. 1977.    See also Correction to "The Shannon sampling theorem—Its various extensions and applications: A tutorial review", Proceedings of the IEEE, 67:695, April 1979 and by Lüke.Hans Dieter Lüke, [tpl]doi-inline|10.1109/35.755459|The Origins of the Sampling Theorem[/tpl], IEEE Communications Magazine, pp.106–108, April 1999.  For example, Lüke points out that H. Raabe, an assistant to Küpfmüller, proved the theorem in his 1939 Ph.D. dissertation; the term Raabe condition came to be associated with the criterion for unambiguous representation (sampling rate greater than twice the bandwidth).
MeijeringErik Meijering, [tpl]doi-inline|10.1109/5.993400|A Chronology of Interpolation From Ancient Astronomy to Modern Signal and Image Processing[/tpl], Proc. IEEE, 90, 2002. mentions several other discoverers and names in a paragraph and pair of footnotes:
 
 As pointed out by Higgins 135, the sampling theorem should really be considered in two parts, as done above: the first stating the fact that a bandlimited function is completely  determined by its samples, the second describing how to reconstruct the function using  its samples. Both parts of the sampling theorem were given in a somewhat different  form by J. M. Whittaker 351, 353 and before him also by Ogura 242. They were probably not aware of the fact that the first part of the theorem had been stated as early as 1897 by Borel 25.27 As we have seen, Borel also used around that time what became known as the cardinal series. However, he appears not to have made the link 135. In later years it became known that the sampling theorem had been presented before Shannon to the Russian communication community by Kotel'nikov 173. In more implicit, verbal form, it had also been described in the German literature by Raabe 257. Several authors 205 have mentioned that Someya 296 introduced the theorem in the Japanese literature parallel to Shannon. In the English literature, Weston 347 introduced it independently of Shannon around the same time.28

 
 27 Several authors, following Black 16, have claimed that this first part of the sampling theorem was stated even earlier by Cauchy, in a paper 41 published in 1841. However, the paper of Cauchy does not contain such a statement, as has been pointed out by Higgins 135.

 
 28 As a consequence of the discovery of the several independent introductions of the sampling theorem, people started to refer to the theorem by including the names of the aforementioned authors, resulting in such catchphrases as “the Whittaker-Kotel’nikov-Shannon (WKS) sampling theorem" 155 or even "the Whittaker-Kotel'nikov-Raabe-Shannon-Someya sampling theorem" 33. To avoid confusion, perhaps the best thing to do is to refer to it as the sampling theorem, "rather than trying to find a title that does justice to all claimants" 136.

===Why Nyquist?===

Exactly how, when, or why Harry Nyquist had his name attached to the sampling theorem remains obscure.  The term Nyquist Sampling Theorem (capitalized thus) appeared as early as 1959 in a book from his former employer, Bell Labs,[tpl]cite book | title = Transmission Systems for Communications | author = Members of the Technical Staff of Bell Telephone Lababoratories | year = 1959 | publisher = AT&T | pages = 26–4 (Vol.2)[/tpl] and appeared again in 1963,[tpl]cite book | title = Theory of Linear Physical Systems | publisher = Wiley | year = 1963 | url = http://books.google.com/books?id=jtI-AAAAIAAJ | author = Ernst Adolph Guillemin [/tpl] and not capitalized in 1965.[tpl]cite book |first=Richard A. |last=Roberts |first2=Ben F. |last2=Barton |title=Theory of Signal Detectability: Composite Deferred Decision Theory |year=1965 [/tpl]  It had been called the Shannon Sampling Theorem as early as 1954,[tpl]cite book |first=Truman S. |last=Gray |title=Applied Electronics: A First Course in Electronics, Electron Tubes, and Associated Circuits |year=1954 [/tpl] but also just the sampling theorem by several other books in the early 1950s.
In 1958, Blackman and Tukey cited Nyquist's 1928 paper as a reference for the sampling theorem of information theory,[tpl]cite book |first=R. B. |last=Blackman |first2=J. W. |last=Tukey |title=The Measurement of Power Spectra : From the Point of View of Communications Engineering |location=New York |publisher=Dover |year=1958 [/tpl] even though that paper does not treat sampling and reconstruction of continuous signals as others did.  Their glossary of terms includes these entries:
Exactly what "Nyquist's result" they are referring to remains mysterious.
When Shannon stated and proved the sampling theorem in his 1949 paper, according to Meijering "he referred to the critical sampling interval T = 1/(2W) as the Nyquist interval corresponding to the band W, in recognition of Nyquist’s discovery of the fundamental importance of this interval in connection with telegraphy."  This explains Nyquist's name on the critical interval, but not on the theorem.
Similarly, Nyquist's name was attached to Nyquist rate in 1953 by Harold S. Black:[tpl]cite book |first=Harold S. |last=Black |title=Modulation Theory |year=1953 [/tpl]
According to the OED, this may be the origin of the term Nyquist rate.  In Black's usage, it is not a sampling rate, but a signaling rate.

==See also==

==Notes==

==References==

==External links==


