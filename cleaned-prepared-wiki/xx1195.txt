[[Chinese room]]

CATEGORIES: Philosophical arguments, Philosophy of artificial intelligence, Thought experiments in philosophy of mind, Thought experiments in philosophy

The Chinese room is a thought experiment presented by John Searle to challenge the claim that it is possible for a digital computer running a program to have a "mind" and "consciousness" in the same sense that people do, simply by virtue of running the right program. According to Searle, when referring to a hypothetical computer program which can be told a story then answer questions about it:  Partisans of strong AI claim that in this question and answer sequence the machine is not only simulating a human ability but also (1) that the machine can literally be said to understand the story and provide the answers to questions, and (2) that what the machine and its program do explains the human ability to understand the story and answer questions about it. 
To contest this view, Searle writes in his first description of the argument: "Suppose that I'm locked in a room and ... that I know no Chinese, either written or spoken". He further supposes that he has a set of rules in English that "enable me to correlate one set of formal symbols with another set of formal symbols", that is, the Chinese characters. These rules allow him to  respond, in written Chinese, to questions, also written in Chinese, in such a way that the posers of the questions – who do understand Chinese – are convinced that Searle can actually understand the Chinese conversation too, even though he cannot. Similarly, he argues that if there is a computer program that allows a computer to carry on an intelligent conversation in written Chinese, the computer executing the program would not understand the conversation either.
The experiment is the centerpiece of Searle's Chinese room argument which holds that a program cannot give a computer a "mind", "understanding" or "consciousness",
Searle's argument first appeared in his paper "Minds, Brains, and Programs", published in Behavioral and Brain Sciences in 1980. It has been widely discussed in the years since.

Chinese room thought experiment

Searle's thought experiment begins with this hypothetical premise: suppose that artificial intelligence research has succeeded in constructing a computer that behaves as if it understands Chinese. It takes Chinese characters as input and, by following the instructions of a computer program, produces other Chinese characters, which it presents as output. Suppose, says Searle, that this computer performs its task so convincingly that it comfortably passes the Turing test: it convinces a human Chinese speaker that the program is itself a live Chinese speaker. To all of the questions that the person asks, it makes appropriate responses, such that any Chinese speaker would be convinced that he or she is talking to another Chinese-speaking human being.
The question Searle wants to answer is this: does the machine literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese? Searle calls the first position "strong AI" and the latter "weak AI".

Searle then supposes that he is in a closed room and has a book with an English version of the computer program, along with sufficient paper, pencils, erasers, and filing cabinets. Searle could receive Chinese characters through a slot in the door, process them according to the program's instructions, and produce Chinese characters as output. If the computer had passed the Turing test this way, it follows, says Searle, that he would do so as well, simply by running the program manually.
Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing a behavior which is then interpreted as demonstrating intelligent conversation. However, Searle would not be able to understand the conversation. ("I don't speak a word of Chinese," he points out.)  Therefore, he argues, it follows that the computer would not be able to understand the conversation either.
Searle argues that without "understanding" (or "intentionality"), we cannot describe what the machine is doing as "thinking" and since it does not think, it does not have a "mind" in anything like the normal sense of the word. Therefore he concludes that "strong AI" is false.

More general context

Apart from controversies about AI is the question of what the Chinese room experiment tells us about consciousness itself. According to Searle:

It is plain that any other method of probing the occupant of a Chinese room has the same difficulties in principle as exchanging questions and answers in Chinese. It is simply not possible to divine whether a conscious agency inhabits the room or some clever simulation. It is the inverse of the brain in a vat situation, and is a strong argument for the hard problem of consciousness being fundamentally insoluble. The argument, to be clear, is not about whether a machine can be conscious, but about whether it (or anything else for that matter) can be shown to be conscious.

History

Gottfried Leibniz made a similar argument in 1714, using the thought experiment of expanding the brain until it was the size of a mill.
The Chinese room was introduced in Searle's 1980 paper "Minds, Brains, and Programs", published in Behavioral and Brain Sciences.
Most of the discussion consists of attempts to refute it. "The overwhelming majority," notes BBS editor Stevan Harnad,
Searle's paper has become "something of a classic in cognitive science," according to Harnad. Varol Akman agrees, and has described his paper as "an exemplar of philosophical clarity and purity".In Akman's review of Mind Design II

Philosophy

Although the Chinese Room argument was originally presented in reaction to the statements of AI researchers, philosophers have come to view it as an important part of the philosophy of mind. It is a challenge to functionalism and the computational theory of mind,

Strong AI

Searle identified a philosophical position he calls "strong AI":
 The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds. 
The definition hinges on the distinction between simulating a mind and actually having a mind. Searle writes that "according to Strong AI, the correct simulation really is a mind. According to Weak AI, the correct simulation is a model of the mind."
The position is implicit in some of the statements of early AI researchers and analysts. For example, in 1955, AI founder Herbert A. Simon declared that "there are now in the world machines that think, that learn and create"Quoted in 
Searle also ascribes the following positions to advocates of strong AI:

Strong AI as computationalism or functionalism

In more recent presentations of the Chinese room argument, Searle has identified "strong AI" as "computer functionalism" (a term he attributes to Daniel Dennett). Functionalism is a position in modern philosophy of mind that holds that we can define mental phenomena (such as beliefs, desires, and perceptions) by describing their functions in relation to each other and to the outside world. Because a computer program can accurately represent functional relationships as relationships between symbols, a computer can have mental phenomena if it runs the right program, according to functionalism.
Stevan Harnad argues that Searle's depictions of strong AI can be reformulated as "recognizable tenets of computationalism, a position (unlike "strong AI") that is actually held by many thinkers, and hence one worth refuting." is the position in the philosophy of mind which argues that the mind can be accurately described as an information-processing system.
Each of the following, according to Harnad, is a "tenet" of computationalism:

Strong AI vs. biological naturalism

Searle holds a philosophical position he calls "biological naturalism": that consciousness
Searle does not disagree that machines can have consciousness and understanding, because, as he writes, "we are precisely such machines". Searle holds that the brain is, in fact, a machine, but the brain gives rise to consciousness and understanding using machinery that is non-computational. If neuroscience is able to isolate the mechanical process that gives rise to consciousness, then Searle grants that it may be possible to create machines that have consciousness and understanding. However, without the specific machinery required, Searle does not believe that consciousness can occur.
Biological naturalism implies that one cannot determine if the experience of consciousness is occurring merely by examining how a system functions, because the specific machinery of the brain is essential. Thus, biological naturalism is directly opposed to both behaviorism and functionalism (including "computer functionalism" or "strong AI").

Computer science

The Chinese room argument is primarily an argument in the philosophy of mind, and both major computer scientists and artificial intelligence researchers consider it irrelevant to their fields. However, several concepts developed by computer scientists are essential to understanding the argument, including symbol processing, Turing machines, Turing completeness, and the Turing test.

Strong AI vs. AI research

Searle's arguments are not usually considered an issue for AI research. Stuart Russell and Peter Norvig observe that most AI researchers "don't care about the strong AI hypothesis—as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence."
Searle's "strong AI" should not be confused with "strong AI" as defined by Ray Kurzweil and other futurists, who use the term to describe machine intelligence that rivals or exceeds human intelligence. Kurzweil is concerned primarily with the amount of intelligence displayed by the machine, whereas Searle's argument sets no limit on this. Searle argues that even a super-intelligent machine would not necessarily have a mind and consciousness.

Symbol processing

The Chinese room (and all modern computers) manipulate physical objects in order to carry out calculations and do simulations. AI researchers Allen Newell and Herbert A. Simon called this kind of machine a physical symbol system. It is also equivalent to the formal systems used in the field of mathematical logic. Searle emphasizes the fact that this kind of symbol manipulation is syntactic (borrowing a term from the study of grammar). The computer manipulates the symbols using a form of syntax rules, without any knowledge of the symbol's semantics (that is, their meaning).

Chinese room as a Turing machine

The Chinese room has a design analogous to that of a modern computer. It has a Von Neumann architecture, which consists of a program (the book of instructions), some memory (the papers and file cabinets), a CPU which follows the instructions (the man), and a means to write symbols in memory (the pencil and eraser). A machine with this design is known in theoretical computer science as "Turing complete", because it has the necessary machinery to carry out any computation that a Turing machine can do, and therefore it is capable of doing a step-by-step simulation of any other digital machine, given enough memory and time. Alan Turing writes, "all digital computers are in a sense equivalent." The widely accepted Church-Turing thesis holds that any function computable by an effective procedure is computable by a Turing machine. In other words, the Chinese room can do whatever any other digital computer can do (albeit much, much more slowly).
There are some critics, such as Hanoch Ben-Yami, who argue that the Chinese room can not simulate all the abilities of a digital computer, such as being able to determine the current time.

Turing test

The Turing test is a test of a machine's ability to exhibit intelligent behaviour. In Alan Turing's original illustrative example, a human judge engages in a natural language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test.

Complete argument

Searle has produced a more formal version of the argument of which the Chinese Room forms a part. He presented the first version in 1984. The version given below is from 1990.
He begins with three axioms:
Searle posits that these lead directly to this conclusion:
This much of the argument is intended to show that artificial intelligence can never produce a machine with a mind by writing programs that manipulate symbols. The remainder of the argument addresses a different issue. Is the human brain running a program? In other words, is the computational theory of mind correct? He begins with an axiom that is intended to express the basic modern scientific consensus about brains and minds:
Searle claims that we can derive "immediately" and "trivially" that:
And from this he derives the further conclusions:

Replies

Replies to Searle's argument may be classified according to what they claim to show:
Some of the arguments (robot and brain simulation, for example) fall into multiple categories.

System and virtual mind replies: finding the mind

These replies attempt to answer the question: since the man in the room doesn't speak Chinese, where is the "mind" that does? These replies address the key ontological issues of mind vs. body and simulation vs. reality. All of the replies that identify the mind in the room are versions of "the system reply".
More sophisticated versions of the system reply try to identify more precisely what "the system" is and they differ in exactly how they describe it. According to these replies, the "mind that speaks Chinese" could be such things as: the "software", a "program", a "running program", a simulation of the "neural correlates of consciousness", the "functional system", a "simulated mind", an "emergent property", or "a virtual mind" (Marvin Minsky's version of the system reply, described below).
These replies provide an explanation of exactly who it is that understands Chinese. If there is something besides the man in the room that can understand Chinese, Searle can't argue that (1) the man doesn't understand Chinese, therefore (2) nothing in the room understands Chinese. This, according to those who make this reply, shows that Searle's argument fails to prove that "strong AI" is false.
However, the replies, by themselves, do not prove that strong AI is true, either: they provide no evidence that the system (or the virtual mind) understands Chinese, other than the hypothetical premise that it passes the Turing Test. As Searle writes "the systems reply simply begs the question by insisting that system must understand Chinese."

Robot and semantics replies: finding the meaning

As far as the person in the room is concerned, the symbols are just meaningless "squiggles." But if the Chinese room really "understands" what it is saying, then the symbols must get their meaning from somewhere. These arguments attempt to connect the symbols to the things they symbolize. These replies address Searle's concerns about intentionality, symbol grounding and syntax vs. semantics.
To each of these suggestions, Searle's response is the same: no matter how much knowledge is written into the program and no matter how the program is connected to the world, he is still in the room manipulating symbols according to rules. His actions are syntactic and this can never explain to him what the symbols stand for. Searle writes "syntax is insufficient for semantics."
However, for those who accept that Searle's actions simulate a mind, separate from his own, the important question is not what the symbols mean to Searle, what is important is what they mean to the virtual mind. While Searle is trapped in the room, the virtual mind is not: it is connected to the outside world through the Chinese speakers it speaks to, through the programmers who gave it world knowledge, and through the cameras and other sensors that roboticists can supply.

Brain simulation and connectionist replies: redesigning the room

These arguments are all versions of the system reply that identify a particular kind of system as being important; they identify some special technology that would create conscious understanding in a machine. (Note that the "robot" and "commonsense knowledge" replies above also specify a certain kind of system as being important.)
 Imagine that instead of a monolingual man in a room shuffling symbols we have the man operate an elaborate set of water pipes with valves connecting them. When the man receives the Chinese symbols, he looks up in the program, written in English, which valves he has to turn on and off. Each water connection corresponds to a synapse in the Chinese brain, and the whole system is rigged up so that after doing all the right firings, that is after turning on all the right faucets, the Chinese answers pop out at the output end of the series of pipes.
 Now where is the understanding in this system? It takes Chinese as input, it simulates the formal structure of the synapses of the Chinese brain, and it gives Chinese as output. But the man certainly doesn't understand Chinese, and neither do the water pipes, and if we are tempted to adopt what I think is the absurd view that somehow the conjunction of man and water pipes understands, remember that in principle the man can internalize the formal structure of the water pipes and do all the "neuron firings" in his imagination.
These arguments (and the robot or commonsense knowledge replies) identify some special technology that would help create conscious understanding in a machine. They may be interpreted in two ways: either they claim (1) this technology is required for consciousness, the Chinese room does not or can not implement this technology, and therefore the Chinese room can not pass the Turing test or (even if it did) it would not have conscious understanding. Or they may be claiming that (2) it is easier to see that the Chinese room has a mind if we visualize this technology is being used to create it.
In the first case, where features like a robot body or a connectionist architecture are required, Searle claims that strong AI (as he understands it) has been abandoned. The Chinese room has all the elements of a Turing complete machine, and thus is capable of simulating any digital computation whatsoever. If Searle's room can't pass the Turing test then there is no other digital technology that could pass the Turing test. If Searle's room could pass the Turing test, but still does not have a mind, then the Turing test is not sufficient to determine if the room has a "mind". Either way, it denies one or the other of the positions Searle thinks of as "strong AI", proving his argument.
The brain arguments in particular deny strong AI if they assume that there is no simpler way to describe the mind than to create a program that is just as mysterious as the brain was. He writes "I thought the whole idea of strong AI was that we don't need to know how the brain works to know how the mind works." If computation does not provide an explanation of the human mind, then strong AI has failed, according to Searle.
Other critics hold that the room as Searle described it does, in fact, have a mind, however they argue that it is difficult to see—Searle's description is correct, but misleading. By redesigning the room more realistically they hope to make this more obvious. In this case, these arguments are being used as appeals to intuition (see next section).
In fact, the room can just as easily be redesigned to weaken our intuitions. Ned Block's "blockhead" argument In the blockhead scenario, the entire mental state is hidden in the letter X, which represents a memory address—a number associated with the next rule. It is hard to visualize that an instant of one's conscious experience can be captured in a single large number, yet this is exactly what "strong AI" claims. On the other hand, such a lookup table would be ridiculously large (probably to the point of being impossible in practice), and the states could therefore be extremely specific.
Searle's argues that however the program is written or however the machine is connected to the world, the mind is being simulated by a simple step by step digital machine (or machines). These machines are always just like the man in the room: they understand nothing and don't speak Chinese. They are merely manipulating symbols without knowing what they mean. Searle writes: "I can have any formal program you like, but I still understand nothing."

Speed and complexity: appeals to intuition

The following arguments (and the intuitive interpretations of the arguments above) do not directly explain how a Chinese speaking mind could exist in Searle's room, or how the symbols he manipulates could become meaningful. However, by raising doubts about Searle's intuitions they support other positions, such as the system and robot replies. These arguments, if accepted, prevent Searle from claiming that his conclusion is obvious by undermining the intuitions that his certainty requires.
Several critics believe that Searle's argument relies entirely on intuitions. Ned Block writes "Searle's argument depends for its force on intuitions that certain entities do not think."Quoted in 
Some of the arguments above also function as appeals to intuition, especially those that are intended to make it seem more plausible that the Chinese room contains a mind, which can include the robot, commonsense knowledge, brain simulation and connectionist replies. Several of the replies above also address the specific issue of complexity. The connectionist reply emphasizes that a working artificial intelligence system would have to be as complex and as interconnected as the human brain. The commonsense knowledge reply emphasizes that any program that passed a Turing test would have to be "an extraordinarily supple, sophisticated, and multilayered system, brimming with 'world knowledge' and meta-knowledge and meta-meta-knowledge", as Daniel Dennett explains.
An especially vivid version of the speed and complexity reply is from Paul and Patricia Churchland. They propose this analogous thought experiment:
Stevan Harnad is critical of speed and complexity replies when they stray beyond addressing our intuitions. He writes "Some have made a cult of speed and timing, holding that, when accelerated to the right speed, the computational may make a phase transition into the mental. It should be clear that is not a counterargument but merely an ad hoc speculation (as is the view that it is all just a matter of ratcheting up to the right degree of 'complexity.')"
Searle accuses his critics of placing too much faith in their own intuitions. Searle argues that anyone who is willing to accept the "system reply" (which asserts that a mind can emerge from "a system" without saying what the system is or how such a thing might give rise to a mind) has been completely misled by their own intuitions, writing that they are "under the grip of an ideology".

Other minds and zombies: meaninglessness

Several replies argue that Searle's argument is irrelevant because his assumptions about the mind and consciousness are faulty. Searle believes that  human beings directly experience their consciousness, intentionality and the nature of the mind every day, and that this experience of consciousness is not open to question.  He writes that we must "presuppose the reality and knowability of the mental." These replies question whether Searle is justified in using his own experience of consciousness to determine that it is more than mechanical symbol processing. In particular, the other minds reply argues that we cannot use our experience of consciousness to answer questions about other minds (even the mind of a computer), and the epiphenomena reply argues that Searle's consciousness does not "exist" in the sense that Searle thinks it does.
Searle disagrees with this analysis and argues that "the study of the mind starts with such facts as that humans have beliefs, while thermostats, telephones, and adding machines don't ... what we wanted to know is what distinguishes the mind from thermostats and livers." He takes it as obvious that we can detect the presence of consciousness and dismisses these replies as being off the point.
Daniel Dennett provides this extension to the "epiphenomena" argument.

Notes

Citations



| last=Ben-Yami | first=Hanoch
| contribution=A Note on the Chinese Room
| year=1993
| journal=Syntese | volume=95 | number=2
| pages=169–72
}}
| last = Block | first = Ned | author-link = Ned Block
| year = 1981
| title = Psychologism and Behaviourism
| journal = The Philosophical Review | volume=90 | issue=1
| pages = 5–43
| doi = 10.2307/2184371 | jstor = 2184371
| url = 
}}
| last=Chalmers | first=David | author-link=David Chalmers
| year=1996
| title=The Conscious Mind: In Search of a Fundamental Theory | publisher=Oxford University Press
}}
| last=Churchland | first=Paul | last2=Churchland
| first2=Patricia | author-link=Paul Churchland | author2-link = Patricia Churchland
| title= Could a machine think?
| journal=Scientific American | volume=262 | issue=1
| date = January 1990
| pages=32–39
| pmid = 2294584
}}
| last=Cole | first=David | author-link=David Cole
| contribution =The Chinese Room Argument
| title= The Stanford Encyclopedia of Philosophy
| date = Fall 2004 | year = 2004
| editor-first = Edward N. | editor-last = Zalta
| url= }}
| last=Dennett | first=Daniel | author-link=Daniel Dennett
| year=1991
| title=Consciousness Explained
| publisher=The Penguin Press
| isbn= 0-7139-9037-6
}}
| last=Dreyfus | first=Hubert | authorlink = Hubert Dreyfus
| year =1979
| title = What Computers Still Can't Do
| publisher = MIT Press | location = New York
| isbn=0-262-04134-0
}}
| last=Fearn | first=Nicholas
| year =2007
| title= The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers
| publisher = Grove Press | location=New York
}}
| last=Harnad | first=Stevan | author-link=Stevan Harnad
| year=2001
| url=
|contribution= What's Wrong and Right About Searle's Chinese Room Argument |editor-last= M.
|editor2-last=Preston|editor2-first= J.
|title=Views into the Chinese Room: New Essays on Searle and Artificial Intelligence
|publisher=Oxford University Press
}}
| last=Harnad | first=Stevan
| author-link=Stevan Harnad
| year=2005
| url=
|contribution= Searle's Chinese Room Argument
| title=Encyclopedia of Philosophy
|publisher= Macmillan}}
| last=Haugeland | first=John | author-link = John Haugeland
| year = 1985
| title = Artificial Intelligence: The Very Idea
| publisher=MIT Press| location= Cambridge, Mass.
| isbn=0-262-08153-9
}}
| last=Haugeland | first=John | author-link = John Haugeland
| year = 1981
| title = Mind Design
| publisher=MIT Press| location= Cambridge, Mass.
| isbn=0-262-08110-5
}}
| last=Hauser | first=Larry | author-link=Larry Hauser
| year=2006
| contribution=Searle's Chinese Room
| title=Internet Encyclopedia of Philosophy
| url= }}
| first = Ray | last = Kurzweil | author-link =Ray Kurzweil
| title = The Singularity is Near
| year = 2005
| publisher = Viking Press
}}
| last=Horst | first= Steven
| contribution =The Computational Theory of Mind
| title= The Stanford Encyclopedia of Philosophy
| date = Fall 2005 | year = 2005
| editor-first = Edward N. | editor-last = Zalta
| url = 
}}
| last=Leibniz | first=Gottfried | authorlink=Gottfried Leibniz
| year=1714
| title=Monadology
| others=George MacDonald Ross (trans.)
| url=
| first = Hans | last = Moravec  | author-link =Hans Moravec
| year = 1988
| title = Mind Children
| publisher = Harvard University Press
}}
| first = Marvin | last = Minsky | author-link = Marvin Minsky
| year = 1980
| title = Decentralized Minds
| journal = Behavioral and Brain Sciences | volume = 3 | issue = 3
| pages=439–40
| doi=10.1017/S0140525X00005914
}}
| last=Motzkin | first=Elhanan
| last2=Searle | first2=John | author2-link=John Searle
| date=February 16, 1989
| contribution=Artificial Intelligence and the Chinese Room: An Exchange
| publisher=New York Review of Books
| url=
}}
| first = Nils | last = Nilsson | author-link = Nils Nilsson (researcher)
| url =
| title =A Short Rebuttal to Searle
| year = 1984
}}
| last=Pinker | first=Steven | author-link=Steven Pinker
| year=1997
| title=How the Mind Works
| publisher=W. W. Norton & Company, Inc. | location=New York, NY
| isbn=0-393-31848-6
}}
| year=2002
|editor-first=John | editor-last= Preston
|editor2-last=Bishop|editor2-first= Mark | editor2-link=Mark Bishop
|title=Views into the Chinese Room: New Essays on Searle and Artificial Intelligence
|publisher=Oxford University Press
| isbn= 0-19-825057-6
}}
| last=Searle | first=John | author-link=John Searle
| year=1983 | contribution=Can Computers Think?
| editor-first=David |editor-last= Chalmers |editor-link=David Chalmers
| title=Philosophy of Mind: Classical and Contemporary Readings
| publisher=Oxford University Press | location=Oxford
| isbn= 0-19-514581-X
| pages= 669–675
}}
| last=Searle | first=John | author-link=John Searle
| year=1984
| title=Minds, Brains and Science: The 1984 Reith Lectures
| publisher=Harvard University Press
| isbn=0-674-57631-4
}} paperback: ISBN 0-674-57633-0.
| last=Searle | first=John | author-link=John Searle
| title=Is the Brain's Mind a Computer Program?
| magazine=Scientific American | volume=262 | issue=1
| date=January 1990| pages=26–31
| pmid=2294583
}}
| last=Searle | first=John | author-link=John Searle
| year=1990
| title = Is the Brain a Digital Computer?
| journal=Proceedings and Addresses of the American Philosophical Association | volume=64 | issue=November
| pages=21–37
| url=
}}
| last=Searle | first=John | author-link=John Searle
| year=1992
| title=The Rediscovery of the Mind
| publisher=M.I.T. Press| location=Cambridge, Massachusetts
}}
| last=Searle | first=John | author-link=John Searle
| year = 1999
| title = Mind, language and society | isbn = 0-465-04521-9
| publisher = Basic Books | location = New York, NY
| oclc = 231867665 43689264 }}
| last=Searle | first=John | author-link=John Searle
| year = 2004
| title = Mind: a brief introduction
| publisher=Oxford University Press, Inc.
| isbn=978-0-19-515733-8
}}
| year = 2009
| contribution=Chinese room argument
| publisher=Scholarpedia | volume=4 | number=8
| url=
| doi=10.4249/scholarpedia.3100
}}

Further reading


