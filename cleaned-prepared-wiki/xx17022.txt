[[Luminosity]]

CATEGORIES: Astrophysics, Physical quantities

In astronomy, luminosity measures the total amount of energy emitted by a star or other astronomical object per unit time. In SI units   this is expressed as joules per second or watts. I.e. The Sun has a total power output of  or 1.00 solar luminosity, the symbol for which is L☉.

Astronomy

thumb|230px|right|Hertzsprung–Russell diagram identifying stellar luminosity as a function of temperature for many stars in our solar neighborhood. although luminosities at other wavelengths are increasingly being used as instruments become available to measure them. A bolometer is the instrument used to measure radiant energy over a wide band by absorption and measurement of heating.  When not qualified, the term "luminosity" means bolometric luminosity, which is measured either in the SI units, watts, or in terms of solar luminosities. A star also radiates neutrinos, which carry off some energy, about 2% in the case of our Sun, producing a stellar wind and contributing to the star's total luminosity.  While bolometers do exist, they cannot be used to measure even the apparent brightness of a star because they are insufficiently sensitive across the electromagnetic spectrum and because most wavelengths do not reach the surface of the Earth.  In practice bolometric magnitudes are measured by taking measurements at certain wavelengths and constructing a model of the total spectrum that is most likely to match those measurements.  In some cases, the process of estimation is extreme, with luminosities being calculated when less than 1% of the energy output is observed, for example with a hot Wolf-Rayet star observed only in the infra-red.
A star's luminosity can be determined from two stellar characteristics: size and effective temperature. The former is typically represented in terms of solar radii, R⊙, while the latter is represented in kelvins, but in most cases neither can be measured directly.  To determine a star's radius, two other metrics are needed: the star's angular diameter and its distance from Earth, often calculated using parallax.  Both can be measured with great accuracy in certain cases, with cool supergiants often having large angular diameters, and some cool evolved stars having masers in their atmospheres that can be used to measure the parallax using VLBI.  However for most stars the angular diameter or parallax, or both, are far below our ability to measure with any certainty.  Since the effective temperature is merely a number that represents the temperature of a black body that would reproduce the luminosity, it obviously cannot be measured directly, but it can be estimated from the spectrum.
An alternate way to measure stellar luminosity is to measure the star's apparent brightness and distance.  A third component needed to derive the luminosity is the degree of interstellar extinction that is present, a condition that usually arises because of gas and dust present in the interstellar medium (ISM), the Earth's atmosphere, and circumstellar matter.  Consequently, one of astronomy's central challenges in determining a star's luminosity is to derive accurate measurements for each of these components, without which an accurate luminosity figure remains elusive.  Extinction can only be measured directly if the actual and observed luminosities are both known, but it can be estimated from the observed colour of a star, using models of the expected level of reddening from the interstellar medium.
In the current system of stellar classification, stars are grouped according to temperature, with the massive, very young and energetic Class O stars boasting temperatures in excess of 30,000 K while the less massive, typically older Class M stars exhibit temperatures less than 3,500 K. Because luminosity is proportional to temperature to the fourth power, the large variation in stellar temperatures produces an even vaster variation in stellar luminosity.  Because the luminosity depends on a high power of the stellar mass, high mass luminous stars have much shorter lifetimes.  The most luminous stars are always young stars, no more than a few million years for the most extreme.  In the Hertzsprung–Russell diagram, the x-axis represents temperature or spectral type while the y-axis represents luminosity or magnitude.  The vast majority of stars are found along the main sequence with blue Class 0 stars found at the top left of the chart while red Class M stars fall to the bottom right.  Certain stars like Deneb and Betelgeuse are found above and to the right of the main sequence, more luminous or cooler than their equivalents on the main sequence.  Increased luminosity at the same temperature, or alternatively cooler temperature at the same luminosity, indicates that these stars are larger than those on the main sequence and they are called giants or supergiants.
Blue and white supergiants are high luminosity stars somewhat cooler than the most luminous main sequence stars.  A star like Deneb, for example, has a luminosity around 200,000 L⊙, a spectral type of A2, and an effective temperature around 8,500 K, meaning it has a radius around 203 R⊙. For comparison, the red supergiant Betelgeuse has a luminosity around 100,000 L⊙, a spectral type of M2, and a temperature around 3,500 K, meaning its radius is about 1,000 R⊙. Red supergiants are the largest type of star, but the most luminous are much smaller and hotter, with temperatures up to 50,000 K and more and luminosities of several million L⊙, meaning their radii are just a few tens of R⊙.  An example is R136a1, over 50,000 K and shining at over 8,000,000 L⊙ (mostly in the UV), it is only 35 R⊙.

Magnitude

Luminosity is an intrinsic measurable property of a star independent of distance.  The concept of magnitude, on the other hand, incorporates distance.  First conceived by the Greek astronomer Hipparchus in the second century BC, the original concept of magnitude grouped stars into six discrete categories depending on how bright they appeared.  The brightest first magnitude stars were twice as bright as the next brightest stars, which were second magnitude; second was twice as bright as third, third twice as bright as fourth and so on down to the faintest stars, which Hipparchus categorized as sixth magnitude. The system was but a simple delineation of stellar brightness into six distinct groups and made no allowance for the variations in brightness within a group.  With the invention of the telescope at the beginning of the seventeenth century, researchers soon realized that there were subtle variations among stars and millions fainter than the sixth magnitude—hence the need for a more sophisticated system to describe a continuous range of values beyond what the naked eye could see.[ref]
</ref>
In 1856 Norman Pogson, noticing that photometric measurements had established first magnitude stars as being about 100 times brighter than sixth magnitude stars, formalized the Hipparchus system by creating a logarithmic scale, with every interval of one magnitude equating to a variation in brightness of 1001/5 or roughly 2.512 times. Consequently, a first magnitude star is about 2.5 times brighter than a second magnitude star, 2.52 brighter than a third magnitude star, 2.53 brighter than a fourth magnitude star, et cetera. Based on this continuous scale, any star with a magnitude between 5.5 and 6.5 is now considered to be sixth magnitude, a star with a magnitude between 4.5 and 5.5 is fifth magnitude and so on.  With this new mathematical rigor, a first magnitude star should then have a magnitude in the range 0.5 to 1.5, thus excluding the nine brightest stars with magnitudes lower than 0.5, as well as the four brightest with negative values. It is customary therefore to extend the definition of a first magnitude star to any star with a magnitude less than 0.5, as can be seen in accompanying table.[ref]
</ref>
