 The sort in step&nbsp;2 is usually done using [[bucket sort]] or [[counting sort]], which are efficient in this case since there are usually only a small number of digits.

===An example===

Original, unsorted list:
Sorting by least significant digit (1s place) gives:
Sorting by next digit (10s place) gives:
Sorting by most significant digit (100s place) gives:
It is important to realize that each of the above steps requires just a single pass over the data, since each item can be placed in its correct bucket without having to be compared with other items.
Some LSD radix sort implementations allocate space for buckets by first counting the number of keys that belong in each bucket before moving keys into those buckets.  The number of times that each digit occurs is stored in an array.  Consider the previous list of keys viewed in a different way:
The first counting pass starts on the least significant digit of each key, producing an array of bucket sizes:
A second counting pass on the next more significant digit of each key will produce an array of bucket sizes:
A third and final counting pass on the most significant digit of each key will produce an array of bucket sizes:
At least one LSD radix sort implementation now counts the number of times that each digit occurs in each column for all columns in a single counting pass. (See the external links section.) Other LSD radix sort implementations allocate space for buckets dynamically as the space is needed.

===Iterative version using queues===

A simple version of an LSD radix sort can be achieved using queues as buckets. The following process is repeated for a number of times equal to the length of the longest key:
While this may not be the most efficient radix sort algorithm, it is relatively simple, and still quite efficient.
During all tests on 100M or fewer random 64-bit integers, qsort algorithm behaves faster.

===Example in C===

===Example in Python===

This example written in the Python programming language will perform the radix sort for any radix (base) of 2 or greater. Simplicity of exposition is chosen over clever programming, and so the log function is used instead of bit shifting techniques.

==Most significant digit radix sorts==

A most significant digit (MSD) radix sort can be used to sort keys in lexicographic order.  Unlike a least significant digit (LSD) radix sort, a most significant digit radix sort does not necessarily preserve the original order of duplicate keys.  An MSD radix sort starts processing the keys from the most significant digit, leftmost digit, to the least significant digit, rightmost digit.  This sequence is opposite that of least significant digit (LSD) radix sorts.  An MSD radix sort stops rearranging the position of a key when the processing reaches a unique prefix of the key.  Some MSD radix sorts use one level of buckets in which to group the keys.  See the counting sort and pigeonhole sort articles.  Other MSD radix sorts use multiple levels of buckets, which form a trie or a path in a trie.  A postman's sort / postal sort is a kind of MSD radix sort.

===Recursion===

A recursively subdividing MSD radix sort algorithm works as follows:

====Implementation====

A two-pass method can be used to first find out how big each bucket needs to be and then place each key (or pointer to the key) into the appropriate bucket.  A single-pass system can also be used, where each bucket is dynamically allocated and resized as needed, but this runs the risk of serious memory fragmentation, discontiguous allocations of memory, which may degrade performance.  This memory fragmentation could be avoided if a fixed allocation of buckets is used for all possible values of a digit, but, for an 8-bit digit, this would require 256 (28) buckets, even if not all of the buckets were used.  So, this approach might use up all available memory quickly and go into paging space, where data is stored and accessed on a hard drive or some other secondary memory device instead of main memory, which would radically degrade performance.  A fixed allocation approach would only make sense if each digit was very small, such as a single bit.

===Recursive forward radix sort example===

Sort the list:
 170, 045, 075, 090, 002, 024, 802, 066
Sorting by least significant digit (1s place) is not needed, as there is no tens bucket with more than one number.  Therefore, the now sorted zero hundreds bucket is concatenated, joined in sequence, with the one hundreds bucket and eight hundreds bucket to give: 002, 024, 045, 066, 075, 090, 170, 802
This example used base ten digits for the sake of readability, but of course binary digits or perhaps bytes might make more sense for a binary computer to process.

===In-place MSD radix sort implementations===

Binary MSD radix sort, also called binary quicksort, can be implemented in-place by splitting the input array into two bins - the 0s bin and the 1s bin.  The 0s bin is grown from the beginning of the array, whereas the 1s bin is grown from the end of the array.  The 0s bin boundary is placed before the first array element.  The 1s bin boundary is placed after the last array element.  The most significant bit of the first array element is examined.  If this bit is a 1, then the first element is swapped with the element in front of the 1s bin boundary (the last element of the array), and the 1s bin is grown by one element by decrementing the 1s boundary array index.  If this bit is a 0, then the first element remains at its current location, and the 0s bin is grown by one element. The next array element examined is the one in front of the 0s bin boundary (i.e. the first element that is not in the 0s bin or the 1s bin).  This process continues until the 0s bin and the 1s bin reach each other. The 0s bin and the 1s bin are then sorted recursively based on the next bit of each array element.  Recursive processing continues until the least significant bit has been used for sorting.R. Sedgewick, "Algorithms in C++", third edition, 1998, p. 424-427V. J. Duvanenko, "In-Place Hybrid Binary-Radix Sort", Dr. Dobb's Journal, 1 October 2009  Handling signed integers requires treating the most significant bit with the opposite sense, followed by unsigned treatment of the rest of the bits.
In-place MSD binary-radix sort can be extended to larger radix and retain in-place capability.  Counting sort is used to determine the size of each bin and their starting index.  Swapping is used to place the current element into its bin, followed by expanding the bin boundary.  As the array elements are scanned the bins are skipped over and only elements between bins are processed, until the entire array has been processed and all elements end up in their respective bins.  The number of bins is the same as the radix used - e.g. 16 bins for 16-Radix.  Each pass is based on a single digit (e.g. 4-bits per digit in the case of 16-Radix), starting from the most significant digit.  Each bin is then processed recursively using the next digit, until all digits have been used for sorting.V. J. Duvanenko, "In-Place Hybrid N-bit-Radix Sort", Dr. Dobb's Journal, November 2009V. J. Duvanenko, "Parallel In-Place Radix Sort Simplified", Dr. Dobb's Journal, January 2011
Neither in-place binary-radix sort nor n-bit-radix sort, discussed in paragraphs above, are stable algorithms.

===Stable MSD radix sort implementations===

MSD Radix Sort can be implemented as a stable algorithm, but requires the use of a memory buffer of the same size as the input array.  This extra memory allows the input buffer to be scanned from the first array element to last, and move the array elements to the destination bins in the same order.  Thus, equal elements will be placed in the memory buffer in the same order they were in the input array.  The MSD-based algorithm uses the extra memory buffer as the output on the first level of recursion, but swaps the input and output on the next level of recursion, to avoid the overhead of copying the output result back to the input buffer.  Each of the bins are recursively processed, as is done for the in-place MSD Radix Sort.  After the sort by the last digit has been completed, the output buffer is checked to see if it is the original input array, and if it's not, then a single copy is performed.  If the digit size is chosen such that the key size divided by the digit size is an even number, the copy at the end is avoided.V. J. Duvanenko, "Stable Hybrid N-bit-Radix Sort", Dr. Dobb's Journal, January 2010

===Hybrid approaches===

Radix sort, such as two pass method where counting sort is used during the first pass of each level of recursion, has a large constant overhead.  Thus, when the bins get small, other sorting algorithms should be used, such as insertion sort.  A good implementation of Insertion sort is fast for small arrays, stable, in-place, and can significantly speed up Radix Sort.

===Application to parallel computing===

Note that this recursive sorting algorithm has particular application to parallel computing, as each of the bins can be sorted independently.  In this case, each bin is passed to the next available processor.  A single processor would be used at the start (the most significant digit). By the second or third digit, all available processors would likely be engaged.  Ideally, as each subdivision is fully sorted, fewer and fewer processors would be utilized.  In the worst case, all of the keys will be identical or nearly identical to each other, with the result that there will be little to no advantage to using parallel computing to sort the keys.
In the top level of recursion, opportunity for parallelism is in the Counting sort portion of the algorithm.  Counting is highly parallel, amenable to the parallel_reduce pattern, and splits the work well across multiple cores until reaching memory bandwidth limit.  This portion of the algorithm has data-independent parallelism.  Processing each bin in subsequent recursion levels is data-dependent, however.  For example, if all keys were of the same value, then there would be only a single bin with any elements in it, and no parallelism would be available.  For random inputs all bins would be near equally populated and a large amount of parallelism opportunity would be available.V. J. Duvanenko, "Parallel In-Place N-bit-Radix Sort", Dr. Dobb's Journal, August 2010
Note that there are faster sorting algorithms available, for example optimal complexity O(log(n)) are those of the Three Hungarians and Richard ColeA. Gibbons and W. Rytter, "Efficient Parallel Algorithms". Cambridge University Press, 1988.H. Casanova et al, "Parallel Algorithms". Chapman & Hall, 2008. and Batcher's bitonic merge sort has an algorithmic complexity of O(log2(n)), all of which have a lower algorithmic time complexity to radix sort on a CREW-PRAM. The fastest known PRAM sorts were described in 1991 by David Powers with a parallelized quicksort that can operate in O(log(n)) time on a CRCW-PRAM with n processors by performing partitioning implicitly, as well as a radixsort that operates using the same trick in O(k), where k is the maximum keylength.David M. W. Powers, Parallelized Quicksort and Radixsort with Optimal Speedup, Proceedings of International Conference on Parallel Computing Technologies. Novosibirsk. 1991. However, neither the PRAM architecture or a single sequential processor can actually be built in a way that will scale without the number of constant fanout gate delays per cycle increasing as O(log(n)), so that in effect a pipelined version of Batcher's bitonic mergesort and the O(log(n)) PRAM sorts are all O(log2(n)) in terms of clock cycles, with Powers acknowledging that Batcher's would have lower constant in terms of gate delays than his Parallel quicksort and radix sort, or Cole's merge sort, for a keylength-independent sorting network of O(nlog2(n)).David M. W. Powers, Parallel Unification: Practical Complexity, Australasian Computer Architecture Workshop, Flinders University, January 1995

===Incremental trie-based radix sort===

Another way to proceed with an MSD radix sort is to use more memory to create a trie to represent the keys and then traverse the trie to visit each key in order.  A depth-first traversal of a trie starting from the root node will visit each key in order.  A depth-first traversal of a trie, or any other kind of acyclic tree structure, is equivalent to traversing a maze via the right-hand rule.
A trie essentially represents a set of strings or numbers, and a radix sort which uses a trie structure is not necessarily stable, which means that the original order of duplicate keys is not necessarily preserved, because a set does not contain duplicate elements.  Additional information will have to be associated with each key to indicate the population count or original order of any duplicate keys in a trie-based radix sort if keeping track of that information is important for a particular application.  It may even be desirable to discard any duplicate strings as the trie creation proceeds if the goal is to find only unique strings in sorted order.  Some people sort a list of strings first and then make a separate pass through the sorted list to discard duplicate strings, which can be slower than using a trie to simultaneously sort and discard duplicate strings in one pass.
One of the advantages of maintaining the trie structure is that the trie makes it possible to determine quickly if a particular key is a member of the set of keys in a time that is proportional to the length of the key, k, in O(k) time, that is independent of the total number of keys.  Determining set membership in a plain list, as opposed to determining set membership in a trie, requires binary search, O(kâ€‰log(n)) time; linear search, O(kn) time; or some other method whose execution time is in some way dependent on the total number, n, of all of the keys in the worst case.  It is sometimes possible to determine set membership in a plain list in O(k) time, in a time that is independent of the total number of keys, such as when the list is known to be in an arithmetic sequence or some other computable sequence.
Maintaining the trie structure also makes it possible to insert new keys into the set incrementally or delete keys from the set incrementally while maintaining sorted order in O(k) time, in a time that is independent of the total number of keys.  In contrast, other radix sorting algorithms must, in the worst case, re-sort the entire list of keys each time that a new key is added or deleted from an existing list, requiring O(kn) time.

====Snow White analogy====

If the nodes were rooms connected by hallways, then here is how Snow White might proceed to visit all of the dwarfs if the place were dark, keeping her right hand on a wall at all times:
These series of steps serve to illustrate the path taken in the trie by Snow White via a depth-first traversal to visit the dwarfs by the ascending order of their names, Bashful, Doc, Dopey, Grumpy, Happy, Sleepy, and Sneezy.  The algorithm for performing some operation on the data associated with each node of a tree first, such as printing the data, and then moving deeper into the tree is called a pre-order traversal, which is a kind of depth-first traversal.  A pre-order traversal is used to process the contents of a trie in ascending order.  If Snow White wanted to visit the dwarfs by the descending order of their names, then she could walk backwards while following the wall with her right hand, or, alternatively, walk forward while following the wall with her left hand. The algorithm for moving deeper into a tree first until no further descent to unvisited nodes is possible and then performing some operation on the data associated with each node is called post-order traversal, which is another kind of depth-first traversal.  A post-order traversal is used to process the contents of a trie in descending order.
The root node of the trie in the diagram essentially represents a null string, an empty string, which can be useful for keeping track of the number of blank lines in a list of words.  The null string can be associated with a circularly linked list with the null string initially as its only member, with the forward and backward pointers both initially pointing to the null string.  The circularly linked list can then be expanded as each new key is inserted into the trie.  The circularly linked list is represented in the following diagram as thick, grey, horizontally linked lines:
If a new key, other than the null string, is inserted into a leaf node of the trie, then the computer can go to the last preceding node where there was a key or a bifurcation to perform a depth-first search to find the lexicographic successor or predecessor of the inserted key for the purpose of splicing the new key into the circularly linked list.  The last preceding node where there was a key or a bifurcation, a fork in the path, is a parent node in the type of trie shown here, where only unique string prefixes are represented as paths in the trie.  If there is already a key associated with the parent node that would have been visited during a movement away from the root during a right-hand, forward-moving, depth-first traversal, then that immediately ends the depth-first search, as that key is the predecessor of the inserted key.  For example, if Bashful is inserted into the trie, then the predecessor is the null string in the parent node, which is the root node in this case.  In other words, if the key that is being inserted is on the leftmost branch of the parent node, then any string contained in the parent node is the lexicographic predecessor of the key that is being inserted, else the lexicographic predecessor of the key that is being inserted exists down the parent node's branch that is immediately to the left of the branch where the new key is being inserted.  For example, if Grumpy were the last key inserted into the trie, then the computer would have a choice of trying to find either the predecessor, Dopey, or the successor, Happy, with a depth-first search starting from the parent node of Grumpy.  With no additional information to indicate which path is longer, the computer might traverse the longer path, D, O, P.  If Dopey were the last key inserted into the trie, then the depth-first search starting from the parent node of Dopey would soon find the predecessor, "Doc", because that would be the only choice.
If a new key is inserted into an internal node, then a depth-first search can be started from the internal node to find the lexicographic successor.  For example, if the literal string "DO" were inserted in the node at the end of the path D, O, then a depth-first search could be started from that internal node to find the successor, "DOC", for the purpose of splicing the new string into the circularly linked list.
Forming the circularly linked list requires more memory but allows the keys to be visited more directly in either ascending or descending order via a linear traversal of the linked list rather than a depth-first traversal of the entire trie.  This concept of a circularly linked trie structure is similar to the concept of a threaded binary tree.  This structure will be called a circularly threaded trie.
When a trie is used to sort numbers, the number representations must all be the same length unless you are willing to perform a breadth-first traversal.  When the number representations will be visited via depth-first traversal, as in the above diagram, the number representations will always be on the leaf nodes of the trie.  Note how similar in concept this particular example of a trie is to the recursive forward radix sort example which involves the use of buckets instead of a trie.  Performing a radix sort with the buckets is like creating a trie and then discarding the non-leaf nodes.

==See also==

==References==

==External links==


