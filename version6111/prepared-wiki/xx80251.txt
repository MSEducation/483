[[Eigenvalue algorithm]]

CATEGORIES: Numerical linear algebra

In numerical analysis, one of the most important problems is designing efficient and stable algorithms for finding the eigenvalues of a matrix.  These eigenvalue algorithms may also find eigenvectors.

==Eigenvalues and eigenvectors==

Given an [tpl]math|n × n[/tpl] square matrix [tpl]math|A[/tpl] of real or complex numbers, an eigenvalue [tpl]math|λ[/tpl] and its associated generalized eigenvector [tpl]math|v[/tpl] are a pair obeying the relation[ref]
</ref>
where [tpl]math|v[/tpl] is a nonzero [tpl]math|n × 1[/tpl] column vector, [tpl]math|I[/tpl] is the [tpl]math|n × n[/tpl] identity matrix, [tpl]math|k[/tpl] is a positive integer, and both [tpl]math|λ[/tpl] and [tpl]math|v[/tpl] are allowed to be complex even when [tpl]math|A[/tpl] is real. When [tpl]math|1=k = 1[/tpl], the vector is called simply an eigenvector, and the pair is called an eigenpair. In this case, [tpl]math|1=Av = λv[/tpl].  Any eigenvalue [tpl]math|λ[/tpl] of [tpl]math|A[/tpl] has ordinaryThe term "ordinary" is used here only to emphasize the distinction between "eigenvector" and "generalized eigenvector". eigenvectors associated to it, for if [tpl]math|k[/tpl] is the smallest integer such that [tpl]math|1=(A - λI)k v = 0[/tpl] for a generalized eigenvector [tpl]math|v[/tpl], then [tpl]math|1=(A - λI)k-1 v[/tpl] is an ordinary eigenvector. The value [tpl]math|k[/tpl] can always be taken as less than or equal to [tpl]math|n[/tpl]. In particular, [tpl]math|1=(A - λI)n v = 0[/tpl] for all generalized eigenvectors [tpl]math|v[/tpl] associated with [tpl]math|λ.[/tpl]
For each eigenvalue [tpl]math|λ[/tpl] of [tpl]math|A[/tpl], the kernel [tpl]math|ker(A - λI)[/tpl] consists of all eigenvectors associated with [tpl]math|λ[/tpl] (along with 0), called the eigenspace of [tpl]math|λ[/tpl], while the vector space [tpl]math|ker((A - λI)n)[/tpl] consists of all generalized eigenvectors, and is called the generalized eigenspace. The geometric multiplicity of [tpl]math|λ[/tpl] is the dimension of its eigenspace. The algebraic multiplicity of [tpl]math|λ[/tpl] is the dimension of its generalized eigenspace. The latter terminology is justified by the equation
Any collection of generalized eigenvectors of distinct eigenvalues is linearly independent, so a basis for all of [tpl]math|''C n[/tpl] can be chosen consisting of generalized eigenvectors. More particularly, this basis [tpl]math|[tpl]([/tpl]'v'i[tpl])[/tpl][tpl]Sup sub|n|i''[tpl]=[/tpl]1[/tpl][/tpl] can be chosen and organized so that
If these basis vectors are placed as the column vectors of a matrix [tpl]math|V [tpl]=[/tpl] v2  ... vn [/tpl], then [tpl]math|V[/tpl] can be used to convert [tpl]math|A[/tpl] to its Jordan normal form:
where the [tpl]math|λi[/tpl] are the eigenvalues, [tpl]math|1=βi = 1[/tpl] if [tpl]math|1=(A - λi+1)vi+1 = vi[/tpl] and [tpl]math|1=βi = 0[/tpl] otherwise.
More generally, if [tpl]math|W[/tpl] is any invertible matrix, and [tpl]math|λ[/tpl] is an eigenvalue of [tpl]math|A[/tpl] with generalized eigenvector [tpl]math|v[/tpl], then [tpl]math|1=(W -1AW - λI )k W -kv = 0[/tpl]. Thus [tpl]math|λ[/tpl] is an eigenvalue of [tpl]math|W -1AW[/tpl] with generalized eigenvector [tpl]math|W -kv[/tpl]. That is, similar matrices have the same eigenvalues.

===Normal, hermitian, and real-symmetric matrices===

The adjoint [tpl]math|M*[/tpl] of a complex matrix [tpl]math|M[/tpl] is the transpose of the conjugate of [tpl]math|M[/tpl]: [tpl]math|1=M * = [tpl]overline|M[/tpl] T[/tpl]. A square matrix [tpl]math|A[/tpl] is called normal if it commutes with its adjoint: [tpl]math|1=A*A = AA*[/tpl]. It is called hermitian if it is equal to its adjoint: [tpl]math|1=A* = A[/tpl]. All hermitian matrices are normal. If [tpl]math|A[/tpl] has only real elements, then the adjoint is just the transpose, and [tpl]math|A[/tpl] is hermitian if and only if it is symmetric. When applied to column vectors, the adjoint can be used to define the canonical inner product on [tpl]math|''C n[/tpl]: [tpl]math|1='w • v = w* v[/tpl].This ordering of the inner product (with the conjugate-linear position on the left), is preferred by physicists. Algebraists often place the conjugate-linear position on the right: [tpl]math|1=w • v = v* w'''[/tpl]. Normal, hermitian, and real-symmetric matrices have several useful properties:
It is possible for a real or complex matrix to have all real eigenvalues without being hermitian. For example, a real triangular matrix has its eigenvalues along its diagonal, but in general is not symmetric.

==Condition number==

Any problem of numeric calculation can be viewed as the evaluation of some function ƒ for some input [tpl]math|x[/tpl]. The condition number [tpl]math|κ(ƒ, x)[/tpl] of the problem is the ratio of the relative error in the function's output to the relative error in the input, and varies with both the function and the input. The condition number describes how error grows during the calculation. Its base-10 logarithm tells how many fewer digits of accuracy exist in the result than existed in the input. The condition number is a best-case scenario. It reflects the instability built into the problem, regardless of how it is solved. No algorithm can ever produce more accurate results than indicated by the condition number, except by chance. However, a poorly designed algorithm may produce significantly worse results. For example, as mentioned below, the problem of finding eigenvalues for normal matrices is always well-conditioned. However, the problem of finding the roots of a polynomial can be very ill-conditioned. Thus eigenvalue algorithms that work by finding the roots of the characteristic polynomial can be ill-conditioned even when the problem is not.
For the problem of solving the linear equation [tpl]math|1=Av = b[/tpl] where [tpl]math|A[/tpl] is invertible, the condition number [tpl]math|1=κ(A-1, b)[/tpl] is given by [tpl]math|1=[tpl]!![/tpl]A[tpl]!![/tpl]op[tpl]!![/tpl]A-1[tpl]!![/tpl]op[/tpl], where [tpl]nowrap|[tpl]!![/tpl]  [tpl]!![/tpl]op[/tpl]  is the operator norm subordinate to the normal Euclidian norm on [tpl]math|''C n[/tpl]. Since this number is independent of [tpl]math|'b'[/tpl] and is the same for [tpl]math|A[/tpl] and [tpl]math|A-1[/tpl], it is usually just called the condition number [tpl]math|κ(A)[/tpl] of the matrix [tpl]math|A[/tpl]. This value [tpl]math|κ(A)[/tpl] is also the absolute value of the ratio of the largest eigenvalue of [tpl]math|A[/tpl] to its smallest. If [tpl]math|A[/tpl] is unitary, then [tpl]math|1=[tpl]!![/tpl]A[tpl]!![/tpl]op = [tpl]!![/tpl]A-1[tpl]!![/tpl]op = 1[/tpl], so [tpl]math|1=κ(A'') = 1[/tpl]. For general matrices, the operator norm is often difficult to calculate. For this reason, other matrix norms are commonly used to estimate the condition number.
For the eigenvalue problem, Bauer and Fike proved that if [tpl]math|λ[/tpl] is an eigenvalue for a diagonalizable [tpl]math|n × n[/tpl] matrix [tpl]math|A[/tpl] with eigenvector matrix [tpl]math|V[/tpl], then the absolute error in calculating [tpl]math|λ[/tpl] is bounded by the product of [tpl]math|κ(V)[/tpl] and the absolute error in [tpl]math|A[/tpl].[ref]
