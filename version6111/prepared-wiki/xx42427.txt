[[Gauss–Markov theorem]]

CATEGORIES: Statistical theorems

In statistics, the Gauss–Markov theorem, named after Carl Friedrich Gauss and Andrey Markov, states that in a linear regression model in which the errors have expectation zero and are uncorrelated and have equal variances, the best linear unbiased estimator (BLUE) of the coefficients is given by the ordinary least squares (OLS) estimator.  Here "best" means giving the lowest variance of the estimate, as compared to other unbiased, linear estimates. The errors don't need to be normal, nor do they need to be independent and identically distributed (only uncorrelated and homoscedastic). The hypothesis that the estimator be unbiased cannot be dropped, since otherwise estimators better than OLS exist. See for examples the James–Stein estimator (which also drops linearity) or ridge regression.

==Statement==

Suppose we have in matrix notation, 
expanding to,
The Gauss–Markov assumptions are
(i.e., all residuals have the same variance; that is "homoscedasticity"), and
The ordinary least squares estimator (OLS) is the function
that minimizes the sum of squares of residuals (misprediction amounts):

==Proof==

==Generalized least squares estimator==

The generalized least squares (GLS) or Aitken estimator extends the Gauss–Markov theorem to the case where the error vector has a non-scalar covariance matrix[tpl]spaced ndash[/tpl]the Aitken estimator is also a BLUE.A. C. Aitken, "On Least Squares and Linear Combinations of Observations", Proceedings of the Royal Society of Edinburgh, 1935, vol. 55, pp. 42–48.

==See also==

===Other unbiased statistics===

==Notes==

==References==

 |authorlink=R. L. Plackett |last=Plackett |first=R.L.
 |year=1950
 |title=Some Theorems in Least Squares
