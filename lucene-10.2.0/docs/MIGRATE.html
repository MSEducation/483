<html>
<head>
<title>Apache Lucene Migration Guide</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<!--
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
    this work for additional information regarding copyright ownership.
    The ASF licenses this file to You under the Apache License, Version 2.0
    the "License"); you may not use this file except in compliance with
    the License.  You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
 -->
<h1 id="apache-lucene-migration-guide">Apache Lucene Migration Guide</h1>
<h2 id="migration-from-lucene-9x-to-lucene-100">Migration from Lucene 9.x to Lucene 10.0</h2>
<h3 id="datainputreadvlong-may-now-read-negative-vlongs">DataInput#readVLong() may now read negative vlongs</h3>
<p><a href="https://issues.apache.org/jira/browse/LUCENE-10376">LUCENE-10376</a> started allowing <code>DataInput#readVLong()</code> to read negative vlongs. In particular, this feature is used by the <code>DataInput#readZLong()</code> method. A practical implication is that <code>DataInput#readVLong()</code> may now read up to 10 bytes, while it would never read more than 9 bytes in Lucene 9.x.</p>
<h3 id="changes-to-datainputreadgroupvint-and-readgroupvints-methods">Changes to DataInput.readGroupVInt and readGroupVInts methods</h3>
<p>As part of <a href="https://github.com/apache/lucene/issues/13820">GITHUB#13820</a>, <a href="https://github.com/apache/lucene/issues/13825">GITHUB#13825</a>, <a href="https://github.com/apache/lucene/issues/13830">GITHUB#13830</a>, this issue corrects DataInput.readGroupVInts to be public and not-final, allowing subclasses to override it. This change also removes the protected DataInput.readGroupVInt method: subclasses should delegate or reimplement it entirely.</p>
<h3 id="opennlp-dependency-upgrade">OpenNLP dependency upgrade</h3>
<p><a href="https://opennlp.apache.org">Apache OpenNLP</a> 2.x opens the door to accessing various models via the ONNX runtime.  To migrate you will need to update any deprecated OpenNLP methods that you may be using.</p>
<h3 id="snowball-dependency-upgrade">Snowball dependency upgrade</h3>
<p>Snowball has folded the &quot;German2&quot; stemmer into their &quot;German&quot; stemmer, so there's no &quot;German2&quot; anymore.  For Lucene APIs (TokenFilter, TokenFilterFactory) that accept String, &quot;German2&quot; will be mapped to &quot;German&quot; to avoid breaking users. If you were previously creating German2Stemmer instances, you'll need to change your code to create GermanStemmer instances instead.  For more information see <a href="https://snowballstem.org/algorithms/german2/stemmer.html">https://snowballstem.org/algorithms/german2/stemmer.html</a></p>
<h3 id="romanian-analysis">Romanian analysis</h3>
<p>RomanianAnalyzer now works with Romanian in its modern unicode form, and normalizes cedilla forms to forms with commas. Both forms are still in use in &quot;the wild&quot;: you should reindex Romanian documents.</p>
<h3 id="indexwriter-requires-a-parent-document-field-in-order-to-use-index-sorting-with-document-blocks-github12829">IndexWriter requires a parent document field in order to use index sorting with document blocks (<a href="https://github.com/apache/lucene/issues/12829">GITHUB#12829</a>)</h3>
<p>For indices newly created as of 10.0.0 onwards, IndexWriter preserves document blocks indexed via IndexWriter#addDocuments or IndexWriter#updateDocuments when index sorting is configured. Document blocks are maintained alongside their parent documents during sort and merge. The internally used parent field must be configured in IndexWriterConfig only if index sorting is used together with documents blocks. See <code>IndexWriterConfig#setParendField</code> for reference.</p>
<h3 id="minor-api-changes-in-matchhighlighter-and-matchregionretriever-github12881">Minor API changes in MatchHighlighter and MatchRegionRetriever. (<a href="https://github.com/apache/lucene/issues/12881">GITHUB#12881</a>)</h3>
<p>The API of interfaces for accepting highlights has changed to allow performance improvements. Look at the issue and the PR diff to get a sense of what's changed (changes are minor).</p>
<h3 id="removed-deprecated-indexsearcherdoc-indexreaderdocument-indexreadergettermvectors-github11998">Removed deprecated IndexSearcher.doc, IndexReader.document, IndexReader.getTermVectors (<a href="https://github.com/apache/lucene/issues/11998">GITHUB#11998</a>)</h3>
<p>The deprecated Stored Fields and Term Vectors apis relied upon threadlocal storage and have been removed.</p>
<p>Instead, call storedFields()/termVectors() to return an instance which can fetch data for multiple documents, and will be garbage-collected as usual.</p>
<p>For example:</p>
<pre><code class="language-java">TopDocs hits = searcher.search(query, 10);
StoredFields storedFields = reader.storedFields();
for (ScoreDoc hit : hits.scoreDocs) {
  Document doc = storedFields.document(hit.doc);
}
</code></pre>
<p>Note that these StoredFields and TermVectors instances should only be consumed in the thread where they were acquired. For instance, it is illegal to share them across threads.</p>
<h3 id="field-can-no-longer-configure-a-tokenstream-independently-from-a-value">Field can no longer configure a TokenStream independently from a value</h3>
<p>Lucene 9.x and earlier versions allowed to set a TokenStream on Field instances independently from a string, binary or numeric value. This is no longer allowed on the base Field class. If you need to replicate this behavior, you need to either provide two fields, one with a TokenStream and another one with a value, or create a sub-class of Field that overrides <code>TokenStream tokenStream(Analyzer, TokenStream)</code> to return a custom TokenStream.</p>
<h3 id="persianstemfilter-is-added-to-persiananalyzer-lucene-10312">PersianStemFilter is added to PersianAnalyzer (<a href="https://issues.apache.org/jira/browse/LUCENE-10312">LUCENE-10312</a>)</h3>
<p>PersianAnalyzer now includes PersianStemFilter, that would change analysis results. If you need the exactly same analysis behaviour as 9.x, clone <code>PersianAnalyzer</code> in 9.x or create custom analyzer by using <code>CustomAnalyzer</code> on your own.</p>
<h3 id="automatonquerycompiledautomatonrunautomatonregexp-no-longer-determinize-lucene-10010">AutomatonQuery/CompiledAutomaton/RunAutomaton/RegExp no longer determinize (<a href="https://issues.apache.org/jira/browse/LUCENE-10010">LUCENE-10010</a>)</h3>
<p>These classes no longer take a <code>determinizeWorkLimit</code> and no longer determinize behind the scenes. It is the responsibility of the caller to call <code>Operations.determinize()</code> for DFA execution.</p>
<h3 id="regexp-optional-complement-syntax-has-been-deprecated">RegExp optional complement syntax has been deprecated</h3>
<p>Support for the optional complement syntax (<code>~</code>) has been deprecated. The <code>COMPLEMENT</code> syntax flag has been removed and replaced by the <code>DEPRECATED_COMPLEMENT</code> flag. Users wanting to enable the deprecated complement support can do so by explicitly passing a syntax flags that has <code>DEPRECATED_COMPLEMENT</code> when creating a <code>RegExp</code>. For example: <code>new RegExp(&quot;~(foo)&quot;, RegExp.DEPRECATED_COMPLEMENT)</code>.</p>
<p>Alternatively, and quite commonly, a more simple <em>complement bracket expression</em>, <code>[^...]</code>, may be a suitable replacement, For example, <code>[^fo]</code> matches any character that is not an <code>f</code> or <code>o</code>.</p>
<h3 id="docvaluesfieldexistsquery-normsfieldexistsquery-and-knnvectorfieldexistsquery-removed-in-favor-of-fieldexistsquery-lucene-10436">DocValuesFieldExistsQuery, NormsFieldExistsQuery and KnnVectorFieldExistsQuery removed in favor of FieldExistsQuery (<a href="https://issues.apache.org/jira/browse/LUCENE-10436">LUCENE-10436</a>)</h3>
<p>These classes have been removed and consolidated into <code>FieldExistsQuery</code>. To migrate, caller simply replace those classes with the new one during object instantiation.</p>
<h3 id="normalizer-and-stemmer-classes-are-now-package-private-lucene-10561">Normalizer and stemmer classes are now package private (<a href="https://issues.apache.org/jira/browse/LUCENE-10561">LUCENE-10561</a>)</h3>
<p>Except for a few exceptions, almost all normalizer and stemmer classes are now package private. If your code depends on constants defined in them, copy the constant values and re-define them in your code.</p>
<h3 id="longrangefacetcounts--doublerangefacetcounts-gettopchildren-behavior-change-lucene-10614">LongRangeFacetCounts / DoubleRangeFacetCounts #getTopChildren behavior change (<a href="https://issues.apache.org/jira/browse/LUCENE-10614">LUCENE-10614</a>)</h3>
<p>The behavior of <code>LongRangeFacetCounts</code>/<code>DoubleRangeFacetCounts</code> <code>#getTopChildren</code> actually returns the top-n ranges ordered by count from 10.0 onwards (as described in the <code>Facets</code> API) instead of returning all ranges ordered by constructor-specified range order. The pre-existing behavior in 9.x and earlier can be retained by migrating to the new <code>Facets#getAllChildren</code> API (<a href="https://issues.apache.org/jira/browse/LUCENE-10550">LUCENE-10550</a>).</p>
<h3 id="sortedsetdocvaluesno-more-ords-removed-lucene-10603">SortedSetDocValues#NO_MORE_ORDS removed (<a href="https://issues.apache.org/jira/browse/LUCENE-10603">LUCENE-10603</a>)</h3>
<p><code>SortedSetDocValues#nextOrd()</code> no longer returns <code>NO_MORE_ORDS</code> when ordinals are exhausted for the currently-positioned document. Callers should instead use <code>SortedSetDocValues#docValueCount()</code> to determine the number of valid ordinals for the currently-positioned document up-front. It is now illegal to call <code>SortedSetDocValues#nextOrd()</code> more than <code>SortedSetDocValues#docValueCount()</code> times for the currently-positioned document (doing so will result in undefined behavior).</p>
<h3 id="iocontext-removed-from-directoryopenchecksuminput-github12027">IOContext removed from Directory#openChecksumInput (<a href="https://github.com/apache/lucene/issues/12027">GITHUB#12027</a>)</h3>
<p><code>Directory#openChecksumInput</code> no longer takes in <code>IOContext</code> as a parameter, and will always use value <code>IOContext.READONCE</code> for opening internally, as that's the only valid usage pattern for checksum input. Callers should remove the parameter when calling this method.</p>
<h3 id="daciukmihovautomatonbuilder-is-renamed-to-stringstoautomaton-and-made-package-private">DaciukMihovAutomatonBuilder is renamed to StringsToAutomaton and made package-private</h3>
<p>The former <code>DaciukMihovAutomatonBuilder#build</code> functionality is exposed through <code>Automata#makeStringUnion</code>. Users should be able to directly migrate to the <code>Automata</code> static method as a 1:1 replacement.</p>
<h3 id="remove-deprecated-indexsearchergetexecutor-github12580">Remove deprecated IndexSearcher#getExecutor (<a href="https://github.com/apache/lucene/issues/12580">GITHUB#12580</a>)</h3>
<p>The deprecated getter for the <code>Executor</code> that was optionally provided to the <code>IndexSearcher</code> constructors has been removed. Users that want to execute concurrent tasks should rely instead on the <code>TaskExecutor</code> that the searcher holds, retrieved via <code>IndexSearcher#getTaskExecutor</code>.</p>
<h3 id="checkindex-params--slow-and--fast-are-deprecated-replaced-by--level-x-github11023">CheckIndex params -slow and -fast are deprecated, replaced by -level X (<a href="https://github.com/apache/lucene/issues/11023">GITHUB#11023</a>)</h3>
<p>The <code>CheckIndex</code> former <code>-fast</code> behaviour of performing checksum checks only, is now the default. Added a new parameter: <code>-level X</code>, to set the detail level of the index check. The higher the value, the more checks are performed. Sample <code>-level</code> usage: <code>1</code> (Default) - Checksum checks only, <code>2</code> - all level 1 checks as well as logical integrity checks, <code>3</code> - all level 2 checks as well as slow checks.</p>
<h3 id="expressions-module-now-uses-methodhandle-and-hidden-classes-github12873">Expressions module now uses <code>MethodHandle</code> and hidden classes (<a href="https://github.com/apache/lucene/issues/12873">GITHUB#12873</a>)</h3>
<p>Custom functions in the expressions module must now be passed in a <code>Map</code> using <code>MethodHandle</code> as values. To convert legacy code using maps of reflective <code>java.lang.reflect.Method</code>, use the converter method <code>JavascriptCompiler#convertLegacyFunctions</code>. This should make the mapping mostly compatible. The use of <code>MethodHandle</code> and <a href="https://openjdk.org/jeps/309">Dynamic Class-File Constants (JEP 309)</a> now also allows to pass private methods or methods from different classloaders. It is also possible to adapt guards or filters using the <code>MethodHandles</code> class.</p>
<p>The new implementation of the Javascript expressions compiler no longer supports use of custom <code>ClassLoader</code>, because it uses the new JDK 15 feature <a href="https://openjdk.org/jeps/371">hidden classes (JEP 371)</a>. Due to the use of <code>MethodHandle</code>, classloader isolation is no longer needed, because JS code can only call MHs that were resolved by the application before using the expressions module.</p>
<h3 id="expressionevaluate-declares-to-throw-ioexception-github12878"><code>Expression#evaluate()</code> declares to throw IOException (<a href="https://github.com/apache/lucene/issues/12878">GITHUB#12878</a>)</h3>
<p>The expressions module has changed the <code>Expression#evaluate()</code> method signature: It now declares that it may throw <code>IOException</code>. This was an oversight because compiled expressions call <code>DoubleValues#doubleValue</code> behind the scenes, which may throw <code>IOException</code> on index problems, bubbling up unexpectedly to the caller.</p>
<h3 id="pathhierarchytokenizer-and-reversepathhierarchytokenizer-do-not-produce-overlapping-tokens">PathHierarchyTokenizer and ReversePathHierarchyTokenizer do not produce overlapping tokens</h3>
<p><code>(Reverse)PathHierarchyTokenizer</code> now produces sequential (instead of overlapping) tokens with accurate offsets, making positional queries and highlighters possible for fields tokenized with this tokenizer.</p>
<h3 id="removed-scorabledocid-github12407">Removed Scorable#docID() (<a href="https://github.com/apache/lucene/issues/12407">GITHUB#12407</a>)</h3>
<p>This method has been removed in order to enable more search-time optimizations. Use the doc ID passed to <code>LeafCollector#collect</code> to know which doc ID is being collected.</p>
<h3 id="scorecachingwrappingscorer-now-wraps-a-leafcollector-instead-of-a-scorable-github12407">ScoreCachingWrappingScorer now wraps a LeafCollector instead of a Scorable (<a href="https://github.com/apache/lucene/issues/12407">GITHUB#12407</a>)</h3>
<p>In order to adapt to the removal of <code>Scorable#docID()</code>, <code>ScoreCachingWrappingScorer</code> now wraps a <code>LeafCollector</code> rather than a <code>Scorable</code>.</p>
<h3 id="some-classes-converted-to-records-classes-github13207">Some classes converted to records classes (<a href="https://github.com/apache/lucene/issues/13207">GITHUB#13207</a>)</h3>
<p>Some classes with only final fields and no programming logic were converted to <code>record</code> classes. Those changes are mostly compatible with Lucene 9.x code (constructors, accessor methods), but record's fields are only available with accessor methods. Some code may need to be refactored to access the members using method calls instead of field accesses. Affected classes:</p>
<ul>
<li><code>IOContext</code>, <code>MergeInfo</code>, and <code>FlushInfo</code> (<a href="https://github.com/apache/lucene/issues/13205">GITHUB#13205</a>)</li>
<li><code>BooleanClause</code> (<a href="https://github.com/apache/lucene/issues/13261">GITHUB#13261</a>)</li>
<li><code>TotalHits</code> (<a href="https://github.com/apache/lucene/issues/13762">GITHUB#13762</a>)</li>
<li><code>TermAndVector</code> (<a href="https://github.com/apache/lucene/issues/13772">GITHUB#13772</a>)</li>
<li>Many basic Lucene classes, including <code>CollectionStatistics</code>, <code>TermStatistics</code> and <code>LeafMetadata</code> (<a href="https://github.com/apache/lucene/issues/13328">GITHUB#13328</a>)</li>
</ul>
<h3 id="boolean-flags-on-iocontext-replaced-with-a-new-readadvice-enum">Boolean flags on IOContext replaced with a new ReadAdvice enum.</h3>
<p>The <code>readOnce</code>, <code>load</code> and <code>random</code> flags on <code>IOContext</code> have been replaced with a new <code>ReadAdvice</code> enum.</p>
<h3 id="iocontextload-and-iocontextread-removed">IOContext.LOAD and IOContext.READ removed</h3>
<p><code>IOContext#LOAD</code> has been removed, it should be replaced with <code>ioContext.withReadAdvice(ReadAdvice.RANDOM_PRELOAD)</code>.</p>
<p><code>IOContext.READ</code> has been removed, it should be replaced with <code>IOContext.DEFAULT</code>.</p>
<h3 id="timelimitingcollector-removed-github13243">TimeLimitingCollector removed (<a href="https://github.com/apache/lucene/issues/13243">GITHUB#13243</a>)</h3>
<p><code>TimeLimitingCollector</code> has been removed, use <code>IndexSearcher#setTimeout(QueryTimeout)</code> to time out queries instead.</p>
<h3 id="indexsearchsearchquery-collector-being-deprecated-in-favor-of-indexsearchersearchquery-collectormanager-lucene-10002">IndexSearch#search(Query, Collector) being deprecated in favor of IndexSearcher#search(Query, CollectorManager) (<a href="https://issues.apache.org/jira/browse/LUCENE-10002">LUCENE-10002</a>)</h3>
<p><code>IndexSearch#search(Query, Collector)</code> is now being deprecated in favor of <code>IndexSearcher#search(Query, CollectorManager)</code>, as <code>CollectorManager</code> implementation would allow taking advantage of intra-query concurrency via its map-reduce API design. To migrate, use a provided <code>CollectorManager</code> implementation that suits your use cases, or change your <code>Collector</code> implementation to follow the new API pattern. The straight forward approach would be to instantiate the single-threaded <code>Collector</code> in a wrapper <code>CollectorManager</code>.</p>
<p>For example</p>
<pre><code class="language-java">public class CustomCollectorManager implements CollectorManager&lt;CustomCollector, List&lt;Object&gt;&gt; {
    @Override
    public CustomCollector newCollector() throws IOException {
      return new CustomCollector();
    }

    @Override
    public List&lt;Object&gt; reduce(Collection&lt;CustomCollector&gt; collectors) throws IOException {
      List&lt;Object&gt; all = new ArrayList&lt;&gt;();
      for (CustomCollector c : collectors) {
        all.addAll(c.getResult());
      }

      return all;
    }
}

List&lt;Object&gt; results = searcher.search(query, new CustomCollectorManager());
</code></pre>
<h3 id="accountable-interface-removed-from-knnvectorsreader-github13255">Accountable interface removed from KnnVectorsReader (<a href="https://github.com/apache/lucene/issues/13255">GITHUB#13255</a>)</h3>
<p><code>KnnVectorsReader</code> objects use small heap memory, so it's not worth maintaining heap usage for them hence removed <code>Accountable</code> interface from <code>KnnVectorsReader</code>.</p>
<h3 id="deprecated-code-removal-github13262">Deprecated code removal (<a href="https://github.com/apache/lucene/issues/13262">GITHUB#13262</a>)</h3>
<ol>
<li><code>IntField(String name, int value)</code>. Use <code>IntField(String, int, Field.Store)</code> with <code>Field.Store#NO</code> instead.</li>
<li><code>DoubleField(String name, double value)</code>. Use <code>DoubleField(String, double, Field.Store)</code> with <code>Field.Store#NO</code> instead.</li>
<li><code>FloatField(String name, float value)</code>. Use <code>FloatField(String, float, Field.Store)</code> with <code>Field.Store#NO</code> instead.</li>
<li><code>LongField(String name, long value)</code>. Use <code>LongField(String, long, Field.Store)</code> with <code>Field.Store#NO</code> instead.</li>
<li><code>LongPoint#newDistanceFeatureQuery(String field, float weight, long origin, long pivotDistance)</code>. Use <code>LongField#newDistanceFeatureQuery</code> instead</li>
<li><code>BooleanQuery#TooManyClauses</code>, <code>BooleanQuery#getMaxClauseCount()</code>, <code>BooleanQuery#setMaxClauseCount()</code>. Use <code>IndexSearcher#TooManyClauses</code>, <code>IndexSearcher#getMaxClauseCount()</code>, <code>IndexSearcher#setMaxClauseCount()</code> instead</li>
<li><code>ByteBuffersDataInput#size()</code>. Use <code>ByteBuffersDataInput#length()</code> instead</li>
<li><code>SortedSetDocValuesFacetField#label</code>. <code>FacetsConfig#pathToString(String[])</code> can be applied to path as a replacement if string path is desired.</li>
</ol>
<h3 id="auto-io-throttling-disabled-by-default-in-concurrentmergescheduler-github13293">Auto I/O throttling disabled by default in ConcurrentMergeScheduler (<a href="https://github.com/apache/lucene/issues/13293">GITHUB#13293</a>)</h3>
<p>ConcurrentMergeScheduler now disables auto I/O throttling by default. There is still some throttling happening at the CPU level, since ConcurrentMergeScheduler has a maximum number of threads it can use, which is only a fraction of the total number of threads of the host by default.</p>
<h3 id="fieldinfoshasvectors-and-fieldinfohasvectors-renamed-to-hastermvectors">FieldInfos#hasVectors and FieldInfo#hasVectors renamed to hasTermVectors</h3>
<p>To reduce confusion between term vectors and numeric vectors, <code>hasVectors</code> has been renamed to <code>hasTermVectors</code>.</p>
<h2 id="migration-from-lucene-90-to-lucene-91">Migration from Lucene 9.0 to Lucene 9.1</h2>
<h3 id="test-framework-package-migration-and-module-lucene-10301">Test framework package migration and module (<a href="https://issues.apache.org/jira/browse/LUCENE-10301">LUCENE-10301</a>)</h3>
<p>The test framework is now a Java module. All the classes have been moved from <code>org.apache.lucene.*</code> to <code>org.apache.lucene.tests.*</code> to avoid package name conflicts with the core module. If you were using the Lucene test framework, the migration should be fairly automatic (package prefix).</p>
<h3 id="minor-syntactical-changes-in-standardqueryparser-lucene-10223">Minor syntactical changes in StandardQueryParser (<a href="https://issues.apache.org/jira/browse/LUCENE-10223">LUCENE-10223</a>)</h3>
<p>Added interval functions and min-should-match support to <code>StandardQueryParser</code>. This means that interval function prefixes (<code>fn:</code>) and the <code>@</code> character after parentheses will parse differently than before. If you need the exact previous behavior, clone the <code>StandardSyntaxParser</code> from the previous version of Lucene and create a custom query parser with that parser.</p>
<h3 id="lucene-core-now-depends-on-javalogging-jul-module-lucene-10342">Lucene Core now depends on java.logging (JUL) module (<a href="https://issues.apache.org/jira/browse/LUCENE-10342">LUCENE-10342</a>)</h3>
<p>Lucene Core now logs certain warnings and errors using Java Util Logging (JUL). It is therefore recommended to install wrapper libraries with JUL logging handlers to feed the log events into your app's own logging system.</p>
<p>Under normal circumstances Lucene won't log anything, but in the case of a problem users should find the logged information in the usual log files.</p>
<p>Lucene also provides a <code>JavaLoggingInfoStream</code> implementation that logs <code>IndexWriter</code> events using JUL.</p>
<p>To feed Lucene's log events into the well-known Log4J system, we refer to the <a href="https://logging.apache.org/log4j/2.x/log4j-jul/index.html">Log4j JDK Logging Adapter</a> in combination with the corresponding system property: <code>java.util.logging.manager=org.apache.logging.log4j.jul.LogManager</code>.</p>
<h3 id="kuromoji-and-nori-analysis-component-constructors-for-custom-dictionaries">Kuromoji and Nori analysis component constructors for custom dictionaries</h3>
<p>The Kuromoji and Nori analysis modules had some way to customize the backing dictionaries by passing a path to file or classpath resources using some inconsistently implemented APIs. This was buggy from the beginning, but some users made use of it. Due to move to Java module system, especially the resource lookup on classpath stopped to work correctly. The Lucene team therefore implemented new APIs to create dictionary implementations with custom data files. Unfortunately there were some shortcomings in the 9.1 version, also when using the now deprecated ctors, so users are advised to upgrade to Lucene 9.2 or stay with 9.0.</p>
<p>See <a href="https://issues.apache.org/jira/browse/LUCENE-10558">LUCENE-10558</a> for more details and workarounds.</p>
<h2 id="migration-from-lucene-8x-to-lucene-90">Migration from Lucene 8.x to Lucene 9.0</h2>
<h3 id="rename-of-binary-artifacts-from--analyzers--to--analysis--lucene-9562">Rename of binary artifacts from '<strong>-analyzers-</strong>' to '<strong>-analysis-</strong>' (<a href="https://issues.apache.org/jira/browse/LUCENE-9562">LUCENE-9562</a>)</h3>
<p>All binary analysis packages (and corresponding Maven artifacts) have been renamed and are now consistent with repository module <code>analysis</code>. You will need to adjust build dependencies to the new coordinates:</p>
<table>
<thead>
<tr><th>Old Artifact Coordinates</th><th>New Artifact Coordinates</th></tr>
</thead>
<tbody>
<tr><td>org.apache.lucene:lucene-analyzers-common</td><td>org.apache.lucene:lucene-analysis-common</td></tr>
<tr><td>org.apache.lucene:lucene-analyzers-icu</td><td>org.apache.lucene:lucene-analysis-icu</td></tr>
<tr><td>org.apache.lucene:lucene-analyzers-kuromoji</td><td>org.apache.lucene:lucene-analysis-kuromoji</td></tr>
<tr><td>org.apache.lucene:lucene-analyzers-morfologik</td><td>org.apache.lucene:lucene-analysis-morfologik</td></tr>
<tr><td>org.apache.lucene:lucene-analyzers-nori</td><td>org.apache.lucene:lucene-analysis-nori</td></tr>
<tr><td>org.apache.lucene:lucene-analyzers-opennlp</td><td>org.apache.lucene:lucene-analysis-opennlp</td></tr>
<tr><td>org.apache.lucene:lucene-analyzers-phonetic</td><td>org.apache.lucene:lucene-analysis-phonetic</td></tr>
<tr><td>org.apache.lucene:lucene-analyzers-smartcn</td><td>org.apache.lucene:lucene-analysis-smartcn</td></tr>
<tr><td>org.apache.lucene:lucene-analyzers-stempel</td><td>org.apache.lucene:lucene-analysis-stempel</td></tr>
</tbody>
</table>
<h3 id="lucenepackage-class-removed-lucene-10260">LucenePackage class removed (<a href="https://issues.apache.org/jira/browse/LUCENE-10260">LUCENE-10260</a>)</h3>
<p><code>LucenePackage</code> class has been removed. The implementation string can be retrieved from <code>Version.getPackageImplementationVersion()</code>.</p>
<h3 id="directory-api-is-now-little-endian-lucene-9047">Directory API is now little-endian (<a href="https://issues.apache.org/jira/browse/LUCENE-9047">LUCENE-9047</a>)</h3>
<p><code>DataOutput</code>'s <code>writeShort()</code>, <code>writeInt()</code>, and <code>writeLong()</code> methods now encode with little-endian byte order. If you have custom subclasses of <code>DataInput</code>/<code>DataOutput</code>, you will need to adjust them from big-endian byte order to little-endian byte order.</p>
<h3 id="nativeunixdirectory-removed-and-replaced-by-directiodirectory-lucene-8982">NativeUnixDirectory removed and replaced by DirectIODirectory (<a href="https://issues.apache.org/jira/browse/LUCENE-8982">LUCENE-8982</a>)</h3>
<p>Java 11 supports to use Direct IO without native wrappers from Java code. <code>NativeUnixDirectory</code> in the misc module was therefore removed and replaced by <code>DirectIODirectory</code>. To use it, you need a JVM and operating system that supports Direct IO.</p>
<h3 id="bm25similaritysetdiscountoverlaps-and-legacybm25similaritysetdiscountoverlaps-methods-removed-lucene-9646">BM25Similarity.setDiscountOverlaps and LegacyBM25Similarity.setDiscountOverlaps methods removed (<a href="https://issues.apache.org/jira/browse/LUCENE-9646">LUCENE-9646</a>)</h3>
<p>The <code>discountOverlaps()</code> parameter for both <code>BM25Similarity</code> and <code>LegacyBM25Similarity</code> is now set by the constructor of those classes.</p>
<h3 id="packages-in-misc-module-are-renamed-lucene-9600">Packages in misc module are renamed (<a href="https://issues.apache.org/jira/browse/LUCENE-9600">LUCENE-9600</a>)</h3>
<p>These packages in the <code>lucene-misc</code> module are renamed:</p>
<table>
<thead>
<tr><th>Old Package Name</th><th>New Package Name</th></tr>
</thead>
<tbody>
<tr><td>org.apache.lucene.document</td><td>org.apache.lucene.misc.document</td></tr>
<tr><td>org.apache.lucene.index</td><td>org.apache.lucene.misc.index</td></tr>
<tr><td>org.apache.lucene.search</td><td>org.apache.lucene.misc.search</td></tr>
<tr><td>org.apache.lucene.store</td><td>org.apache.lucene.misc.store</td></tr>
<tr><td>org.apache.lucene.util</td><td>org.apache.lucene.misc.util</td></tr>
</tbody>
</table>
<p>The following classes were moved to the <code>lucene-core</code> module:</p>
<ul>
<li>org.apache.lucene.document.InetAddressPoint</li>
<li>org.apache.lucene.document.InetAddressRange</li>
</ul>
<h3 id="packages-in-sandbox-module-are-renamed-lucene-9319">Packages in sandbox module are renamed (<a href="https://issues.apache.org/jira/browse/LUCENE-9319">LUCENE-9319</a>)</h3>
<p>These packages in the <code>lucene-sandbox</code> module are renamed:</p>
<table>
<thead>
<tr><th>Old Package Name</th><th>New Package Name</th></tr>
</thead>
<tbody>
<tr><td>org.apache.lucene.codecs</td><td>org.apache.lucene.sandbox.codecs</td></tr>
<tr><td>org.apache.lucene.document</td><td>org.apache.lucene.sandbox.document</td></tr>
<tr><td>org.apache.lucene.search</td><td>org.apache.lucene.sandbox.search</td></tr>
</tbody>
</table>
<h3 id="backward-codecs-are-renamed-lucene-9318">Backward codecs are renamed (<a href="https://issues.apache.org/jira/browse/LUCENE-9318">LUCENE-9318</a>)</h3>
<p>These packages in the <code>lucene-backwards-codecs</code> module are renamed:</p>
<table>
<thead>
<tr><th>Old Package Name</th><th>New Package Name</th></tr>
</thead>
<tbody>
<tr><td>org.apache.lucene.codecs</td><td>org.apache.lucene.backward_codecs</td></tr>
</tbody>
</table>
<h3 id="japanesepartofspeechstopfilterfactory-loads-default-stop-tags-if-tags-argument-not-specified-lucene-9567">JapanesePartOfSpeechStopFilterFactory loads default stop tags if &quot;tags&quot; argument not specified (<a href="https://issues.apache.org/jira/browse/LUCENE-9567">LUCENE-9567</a>)</h3>
<p>Previously, <code>JapanesePartOfSpeechStopFilterFactory</code> added no filter if <code>args</code> didn't include &quot;tags&quot;. Now, it will load the default stop tags returned by <code>JapaneseAnalyzer.getDefaultStopTags()</code> (i.e. the tags from<code>stoptags.txt</code> in the <code>lucene-analyzers-kuromoji</code> jar.)</p>
<h3 id="icucollationkeyanalyzer-is-renamed-lucene-9558">ICUCollationKeyAnalyzer is renamed (<a href="https://issues.apache.org/jira/browse/LUCENE-9558">LUCENE-9558</a>)</h3>
<p>These packages in the <code>lucene-analysis-icu</code> module are renamed:</p>
<table>
<thead>
<tr><th>Old Package Name</th><th>New Package Name</th></tr>
</thead>
<tbody>
<tr><td>org.apache.lucene.collation</td><td>org.apache.lucene.analysis.icu</td></tr>
</tbody>
</table>
<h3 id="base-and-concrete-analysis-factories-are-moved--package-renamed-lucene-9317">Base and concrete analysis factories are moved / package renamed (<a href="https://issues.apache.org/jira/browse/LUCENE-9317">LUCENE-9317</a>)</h3>
<p>Base analysis factories are moved to <code>lucene-core</code>, also their package names are renamed.</p>
<table>
<thead>
<tr><th>Old Class Name</th><th>New Class Name</th></tr>
</thead>
<tbody>
<tr><td>org.apache.lucene.analysis.util.TokenizerFactory</td><td>org.apache.lucene.analysis.TokenizerFactory</td></tr>
<tr><td>org.apache.lucene.analysis.util.CharFilterFactory</td><td>org.apache.lucene.analysis.CharFilterFactory</td></tr>
<tr><td>org.apache.lucene.analysis.util.TokenFilterFactory</td><td>org.apache.lucene.analysis.TokenizerFactory</td></tr>
</tbody>
</table>
<p>The service provider files placed in <code>META-INF/services</code> for custom analysis factories should be renamed as follows:</p>
<ul>
<li>META-INF/services/org.apache.lucene.analysis.TokenizerFactory</li>
<li>META-INF/services/org.apache.lucene.analysis.CharFilterFactory</li>
<li>META-INF/services/org.apache.lucene.analysis.TokenFilterFactory</li>
</ul>
<p><code>StandardTokenizerFactory</code> is moved to <code>lucene-core</code> module.</p>
<p>The <code>org.apache.lucene.analysis.standard</code> package in <code>lucene-analysis-common</code> module is split into <code>org.apache.lucene.analysis.classic</code> and <code>org.apache.lucene.analysis.email</code>.</p>
<h3 id="regexpquery-now-rejects-invalid-backslashes-lucene-9370">RegExpQuery now rejects invalid backslashes (<a href="https://issues.apache.org/jira/browse/LUCENE-9370">LUCENE-9370</a>)</h3>
<p>We now follow the <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#bs">Java rules</a> for accepting backslashes. Alphabetic characters other than s, S, w, W, d or D that are preceded by a backslash are considered illegal syntax and will throw an exception.</p>
<h3 id="regexp-certain-regular-expressions-now-match-differently-lucene-9336">RegExp certain regular expressions now match differently (<a href="https://issues.apache.org/jira/browse/LUCENE-9336">LUCENE-9336</a>)</h3>
<p>The commonly used regular expressions \w \W \d \D \s and \S now work the same way <a href="https://docs.oracle.com/javase/tutorial/essential/regex/pre_char_classes.html#CHART">Java Pattern</a> matching works. Previously these expressions were (mis)interpreted as searches for the literal characters w, d, s etc.</p>
<h3 id="ngramfilterfactory-keepshortterm-option-was-fixed-to-preserveoriginal-lucene-9259">NGramFilterFactory &quot;keepShortTerm&quot; option was fixed to &quot;preserveOriginal&quot; (<a href="https://issues.apache.org/jira/browse/LUCENE-9259">LUCENE-9259</a>)</h3>
<p>The factory option name to output the original term was corrected in accordance with its Javadoc.</p>
<h3 id="indexmergetool-defaults-changes-lucene-9206">IndexMergeTool defaults changes (<a href="https://issues.apache.org/jira/browse/LUCENE-9206">LUCENE-9206</a>)</h3>
<p>This command-line tool no longer forceMerges to a single segment. Instead, by default it just follows (configurable) merge policy. If you really want to merge to a single segment, you can pass <code>-max-segments 1</code>.</p>
<h3 id="fst-builder-is-renamed-fstcompiler-with-fluent-style-builder-lucene-9089">FST Builder is renamed FSTCompiler with fluent-style Builder (<a href="https://issues.apache.org/jira/browse/LUCENE-9089">LUCENE-9089</a>)</h3>
<p>Simply use <code>FSTCompiler</code> instead of the previous <code>Builder</code>. Use either the simple constructor with default settings, or the <code>FSTCompiler.Builder</code> to tune and tweak any parameter.</p>
<h3 id="kuromoji-user-dictionary-now-forbids-illegal-segmentation-lucene-8933">Kuromoji user dictionary now forbids illegal segmentation (<a href="https://issues.apache.org/jira/browse/LUCENE-8933">LUCENE-8933</a>)</h3>
<p>User dictionary now strictly validates if the (concatenated) segment is the same as the surface form. This change avoids unexpected runtime exceptions or behaviours. For example, these entries are not allowed at all and an exception is thrown when loading the dictionary file.</p>
<pre><code># concatenated &quot;日本経済新聞&quot; does not match the surface form &quot;日経新聞&quot;
日経新聞,日本 経済 新聞,ニホン ケイザイ シンブン,カスタム名詞

# concatenated &quot;日経新聞&quot; does not match the surface form &quot;日本経済新聞&quot;
日本経済新聞,日経 新聞,ニッケイ シンブン,カスタム名詞
</code></pre>
<h3 id="japanesetokenizer-no-longer-emits-original-compound-tokens-by-default-when-the-mode-is-not-normal-lucene-9123">JapaneseTokenizer no longer emits original (compound) tokens by default when the mode is not NORMAL (<a href="https://issues.apache.org/jira/browse/LUCENE-9123">LUCENE-9123</a>)</h3>
<p><code>JapaneseTokenizer</code> and <code>JapaneseAnalyzer</code> no longer emits original tokens when <code>discardCompoundToken</code> option is not specified. The constructor option has been introduced since Lucene 8.5.0, and the default value is changed to <code>true</code>.</p>
<p>When given the text &quot;株式会社&quot;, JapaneseTokenizer (mode != NORMAL) emits decompounded tokens &quot;株式&quot; and &quot;会社&quot; only and no longer outputs the original token &quot;株式会社&quot; by default. To output original tokens, <code>discardCompoundToken</code> option should be explicitly set to <code>false</code>. Be aware that if this option is set to <code>false</code>, <code>SynonymFilter</code> or <code>SynonymGraphFilter</code> does not work correctly (see <a href="https://issues.apache.org/jira/browse/LUCENE-9173">LUCENE-9173</a>).</p>
<h3 id="analysis-factories-now-have-customizable-symbolic-names-lucene-8778-and-need-additional-no-arg-constructor-lucene-9281">Analysis factories now have customizable symbolic names (<a href="https://issues.apache.org/jira/browse/LUCENE-8778">LUCENE-8778</a>) and need additional no-arg constructor (<a href="https://issues.apache.org/jira/browse/LUCENE-9281">LUCENE-9281</a>)</h3>
<p>The SPI names for concrete subclasses of <code>TokenizerFactory</code>, <code>TokenFilterFactory</code>, and <code>CharfilterFactory</code> are no longer derived from their class name. Instead, each factory must have a static &quot;NAME&quot; field like this:</p>
<pre><code class="language-java">    /** o.a.l.a.standard.StandardTokenizerFactory's SPI name */
    public static final String NAME = &quot;standard&quot;;
</code></pre>
<p>A factory can be resolved/instantiated with its <code>NAME</code> by using methods such as <code>TokenizerFactory.lookupClass(String)</code> or <code>TokenizerFactory.forName(String, Map&lt;String,String&gt;)</code>.</p>
<p>If there are any user-defined factory classes that don't have proper <code>NAME</code> field, an exception will be thrown when (re)loading factories. e.g., when calling <code>TokenizerFactory.reloadTokenizers(ClassLoader)</code>.</p>
<p>In addition starting all factories need to implement a public no-arg constructor, too. The reason for this change comes from the fact that Lucene now uses <code>java.util.ServiceLoader</code> instead its own implementation to load the factory classes to be compatible with Java Module System changes (e.g., load factories from modules). In the future, extensions to Lucene developed on the Java Module System may expose the factories from their <code>module-info.java</code> file instead of <code>META-INF/services</code>.</p>
<p>This constructor is never called by Lucene, so by default it throws an <code>UnsupportedOperationException</code>. User-defined factory classes should implement it in the following way:</p>
<pre><code class="language-java">    /** Default ctor for compatibility with SPI */
    public StandardTokenizerFactory() {
      throw defaultCtorException();
    }
</code></pre>
<p>(<code>defaultCtorException()</code> is a protected static helper method)</p>
<h3 id="termsenum-is-now-fully-abstract-lucene-8292-lucene-8662">TermsEnum is now fully abstract (<a href="https://issues.apache.org/jira/browse/LUCENE-8292">LUCENE-8292</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-8662">LUCENE-8662</a>)</h3>
<p><code>TermsEnum</code> has been changed to be fully abstract, so non-abstract subclasses must implement all its methods. Non-Performance critical <code>TermsEnum</code>s can use <code>BaseTermsEnum</code> as a base class instead. The change was motivated by several performance issues with <code>FilterTermsEnum</code> that caused significant slowdowns and massive memory consumption due to not delegating all method from <code>TermsEnum</code>.</p>
<h3 id="ramdirectory-ramfile-raminputstream-ramoutputstream-removed-lucene-8474">RAMDirectory, RAMFile, RAMInputStream, RAMOutputStream removed (<a href="https://issues.apache.org/jira/browse/LUCENE-8474">LUCENE-8474</a>)</h3>
<p>RAM-based directory implementation have been removed. <code>ByteBuffersDirectory</code> can be used as a RAM-resident replacement, although it is discouraged in favor of the default <code>MMapDirectory</code>.</p>
<h3 id="similaritysimscorercomputexxxfactor-methods-removed-lucene-8014">Similarity.SimScorer.computeXXXFactor methods removed (<a href="https://issues.apache.org/jira/browse/LUCENE-8014">LUCENE-8014</a>)</h3>
<p><code>SpanQuery</code> and <code>PhraseQuery</code> now always calculate their slops as <code>(1.0 / (1.0 + distance))</code>.  Payload factor calculation is performed by <code>PayloadDecoder</code> in the <code>lucene-queries</code> module.</p>
<h3 id="scorer-must-produce-positive-scores-lucene-7996">Scorer must produce positive scores (<a href="https://issues.apache.org/jira/browse/LUCENE-7996">LUCENE-7996</a>)</h3>
<p><code>Scorer</code>s are no longer allowed to produce negative scores. If you have custom query implementations, you should make sure their score formula may never produce negative scores.</p>
<p>As a side-effect of this change, negative boosts are now rejected and <code>FunctionScoreQuery</code> maps negative values to 0.</p>
<h3 id="customscorequery-boostedquery-and-boostingquery-removed-lucene-8099">CustomScoreQuery, BoostedQuery and BoostingQuery removed (<a href="https://issues.apache.org/jira/browse/LUCENE-8099">LUCENE-8099</a>)</h3>
<p>Instead use <code>FunctionScoreQuery</code> and a <code>DoubleValuesSource</code> implementation.  <code>BoostedQuery</code> and <code>BoostingQuery</code> may be replaced by calls to <code>FunctionScoreQuery.boostByValue()</code> and <code>FunctionScoreQuery.boostByQuery()</code>.  To replace more complex calculations in <code>CustomScoreQuery</code>, use the <code>lucene-expressions</code> module:</p>
<pre><code class="language-java">SimpleBindings bindings = new SimpleBindings();
bindings.add(&quot;score&quot;, DoubleValuesSource.SCORES);
bindings.add(&quot;boost1&quot;, DoubleValuesSource.fromIntField(&quot;myboostfield&quot;));
bindings.add(&quot;boost2&quot;, DoubleValuesSource.fromIntField(&quot;myotherboostfield&quot;));
Expression expr = JavascriptCompiler.compile(&quot;score * (boost1 + ln(boost2))&quot;);
FunctionScoreQuery q = new FunctionScoreQuery(inputQuery, expr.getDoubleValuesSource(bindings));
</code></pre>
<h3 id="indexoptions-can-no-longer-be-changed-dynamically-lucene-8134">IndexOptions can no longer be changed dynamically (<a href="https://issues.apache.org/jira/browse/LUCENE-8134">LUCENE-8134</a>)</h3>
<p>Changing <code>IndexOptions</code> for a field on the fly will now result into an <code>IllegalArgumentException</code>. If a field is indexed (<code>FieldType.indexOptions() != IndexOptions.NONE</code>) then all documents must have the same index options for that field.</p>
<h3 id="indexsearchercreatenormalizedweight-removed-lucene-8242">IndexSearcher.createNormalizedWeight() removed (<a href="https://issues.apache.org/jira/browse/LUCENE-8242">LUCENE-8242</a>)</h3>
<p>Instead use <code>IndexSearcher.createWeight()</code>, rewriting the query first, and using a boost of <code>1f</code>.</p>
<h3 id="memory-codecs-removed-lucene-8267">Memory codecs removed (<a href="https://issues.apache.org/jira/browse/LUCENE-8267">LUCENE-8267</a>)</h3>
<p>Memory codecs (<code>MemoryPostingsFormat</code>, <code>MemoryDocValuesFormat</code>) have been removed from the codebase.</p>
<h3 id="direct-doc-value-format-removed-lucene-8917">Direct doc-value format removed (<a href="https://issues.apache.org/jira/browse/LUCENE-8917">LUCENE-8917</a>)</h3>
<p>The <code>Direct</code> doc-value format has been removed from the codebase.</p>
<h3 id="querycachingpolicyalways-cache-removed-lucene-8144">QueryCachingPolicy.ALWAYS_CACHE removed (<a href="https://issues.apache.org/jira/browse/LUCENE-8144">LUCENE-8144</a>)</h3>
<p>Caching everything is discouraged as it disables the ability to skip non-interesting documents. <code>ALWAYS_CACHE</code> can be replaced by a <code>UsageTrackingQueryCachingPolicy</code> with an appropriate config.</p>
<h3 id="english-stopwords-are-no-longer-removed-by-default-in-standardanalyzer-lucene-7444">English stopwords are no longer removed by default in StandardAnalyzer (<a href="https://issues.apache.org/jira/browse/LUCENE-7444">LUCENE-7444</a>)</h3>
<p>To retain the old behaviour, pass <code>EnglishAnalyzer.ENGLISH_STOP_WORDS_SET</code> as an argument to the constructor</p>
<h3 id="standardanalyzerenglish-stop-words-set-has-been-moved">StandardAnalyzer.ENGLISH_STOP_WORDS_SET has been moved</h3>
<p>English stop words are now defined in <code>EnglishAnalyzer.ENGLISH_STOP_WORDS_SET</code> in the <code>analysis-common</code> module.</p>
<h3 id="topdocsmaxscore-removed">TopDocs.maxScore removed</h3>
<p><code>TopDocs.maxScore</code> is removed. <code>IndexSearcher</code> and <code>TopFieldCollector</code> no longer have an option to compute the maximum score when sorting by field. If you need to know the maximum score for a query, the recommended approach is to run a separate query:</p>
<pre><code class="language-java">  TopDocs topHits = searcher.search(query, 1);
  float maxScore = topHits.scoreDocs.length == 0 ? Float.NaN : topHits.scoreDocs[0].score;
</code></pre>
<p>Thanks to other optimizations that were added to Lucene 8, this query will be able to efficiently select the top-scoring document without having to visit all matches.</p>
<h3 id="topfieldcollector-always-assumes-fillfieldstrue">TopFieldCollector always assumes fillFields=true</h3>
<p>Because filling sort values doesn't have a significant overhead, the <code>fillFields</code> option has been removed from <code>TopFieldCollector</code> factory methods. Everything behaves as if it was previously set to <code>true</code>.</p>
<h3 id="topfieldcollector-no-longer-takes-a-trackdocscores-option">TopFieldCollector no longer takes a trackDocScores option</h3>
<p>Computing scores at collection time is less efficient than running a second request in order to only compute scores for documents that made it to the top hits. As a consequence, the <code>trackDocScores</code> option has been removed and can be replaced with the new <code>TopFieldCollector.populateScores()</code> helper method.</p>
<h3 id="indexsearchersearchafter-may-return-lower-bounds-of-the-hit-count-and-topdocstotalhits-is-no-longer-a-long">IndexSearcher.search(After) may return lower bounds of the hit count and TopDocs.totalHits is no longer a long</h3>
<p>Lucene 8 received optimizations for collection of top-k matches by not visiting all matches. However these optimizations won't help if all matches still need to be visited in order to compute the total number of hits. As a consequence, <code>IndexSearcher</code>'s <code>search()</code> and <code>searchAfter()</code> methods were changed to only count hits accurately up to 1,000, and <code>Topdocs.totalHits</code> was changed from a <code>long</code> to an object that says whether the hit count is accurate or a lower bound of the actual hit count.</p>
<h3 id="ramdirectory-ramfile-raminputstream-ramoutputstream-are-deprecated-lucene-8467-lucene-8438">RAMDirectory, RAMFile, RAMInputStream, RAMOutputStream are deprecated (<a href="https://issues.apache.org/jira/browse/LUCENE-8467">LUCENE-8467</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-8438">LUCENE-8438</a>)</h3>
<p>This RAM-based directory implementation is an old piece of code that uses inefficient thread synchronization primitives and can be confused as &quot;faster&quot; than the NIO-based <code>MMapDirectory</code>. It is deprecated and scheduled for removal in future versions of Lucene.</p>
<h3 id="leafcollectorsetscorer-now-takes-a-scorable-rather-than-a-scorer-lucene-6228">LeafCollector.setScorer() now takes a Scorable rather than a Scorer (<a href="https://issues.apache.org/jira/browse/LUCENE-6228">LUCENE-6228</a>)</h3>
<p><code>Scorer</code> has a number of methods that should never be called from <code>Collector</code>s, for example those that advance the underlying iterators.  To hide these, <code>LeafCollector.setScorer()</code> now takes a <code>Scorable</code>, an abstract class that scorers can extend, with methods <code>docId()</code> and <code>score()</code>.</p>
<h3 id="scorers-must-have-non-null-weights">Scorers must have non-null Weights</h3>
<p>If a custom <code>Scorer</code> implementation does not have an associated <code>Weight</code>, it can probably be replaced with a <code>Scorable</code> instead.</p>
<h3 id="suggesters-now-return-long-instead-of-long-for-weight-during-indexing-and-double-instead-of-long-at-suggest-time">Suggesters now return Long instead of long for weight() during indexing, and double instead of long at suggest time</h3>
<p>Most code should just require recompilation, though possibly requiring some added casts.</p>
<h3 id="tokenstreamcomponents-is-now-final">TokenStreamComponents is now final</h3>
<p>Instead of overriding <code>TokenStreamComponents.setReader()</code> to customise analyzer initialisation, you should now pass a <code>Consumer&lt;Reader&gt;</code> instance to the <code>TokenStreamComponents</code> constructor.</p>
<h3 id="lowercasetokenizer-and-lowercasetokenizerfactory-have-been-removed">LowerCaseTokenizer and LowerCaseTokenizerFactory have been removed</h3>
<p><code>LowerCaseTokenizer</code> combined tokenization and filtering in a way that broke token normalization, so they have been removed. Instead, use a <code>LetterTokenizer</code> followed by a <code>LowerCaseFilter</code>.</p>
<h3 id="chartokenizer-no-longer-takes-a-normalizer-function">CharTokenizer no longer takes a normalizer function</h3>
<p><code>CharTokenizer</code> now only performs tokenization. To perform any type of filtering use a <code>TokenFilter</code> chain as you would with any other <code>Tokenizer</code>.</p>
<h3 id="highlighter-and-fastvectorhighlighter-no-longer-support-toparenttochildblockjoinquery">Highlighter and FastVectorHighlighter no longer support ToParent/ToChildBlockJoinQuery</h3>
<p>Both <code>Highlighter</code> and <code>FastVectorHighlighter</code> need a custom <code>WeightedSpanTermExtractor</code> or <code>FieldQuery</code>, respectively, in order to support <code>ToParentBlockJoinQuery</code>/<code>ToChildBlockJoinQuery</code>.</p>
<h3 id="multitermawarecomponent-replaced-by-charfilterfactorynormalize-and-tokenfilterfactorynormalize">MultiTermAwareComponent replaced by CharFilterFactory.normalize() and TokenFilterFactory.normalize()</h3>
<p>Normalization is now type-safe, with <code>CharFilterFactory.normalize()</code> returning a <code>Reader</code> and <code>TokenFilterFactory.normalize()</code> returning a <code>TokenFilter</code>.</p>
<h3 id="k11-constant-factor-removed-from-bm25-similarity-numerator-lucene-8563">k1+1 constant factor removed from BM25 similarity numerator (<a href="https://issues.apache.org/jira/browse/LUCENE-8563">LUCENE-8563</a>)</h3>
<p>Scores computed by the <code>BM25Similarity</code> are lower than previously as the <code>k1+1</code> constant factor was removed from the numerator of the scoring formula. Ordering of results is preserved unless scores are computed from multiple fields using different similarities. The previous behaviour is now exposed by the <code>LegacyBM25Similarity</code> class which can be found in the lucene-misc jar.</p>
<h3 id="indexwritermaxdocnumdocs-removed-in-favor-of-indexwritergetdocstats">IndexWriter.maxDoc()/numDocs() removed in favor of IndexWriter.getDocStats()</h3>
<p><code>IndexWriter.getDocStats()</code> should be used instead of <code>maxDoc()</code> / <code>numDocs()</code> which offers a consistent view on document stats. Previously calling two methods in order to get point in time stats was subject to concurrent changes.</p>
<h3 id="maxclausescount-moved-from-booleanquery-to-indexsearcher-lucene-8811">maxClausesCount moved from BooleanQuery To IndexSearcher (<a href="https://issues.apache.org/jira/browse/LUCENE-8811">LUCENE-8811</a>)</h3>
<p><code>IndexSearcher</code> now performs max clause count checks on all types of queries (including BooleanQueries). This led to a logical move of the clauses count from <code>BooleanQuery</code> to <code>IndexSearcher</code>.</p>
<h3 id="topdocsmerge-shall-no-longer-allow-setting-of-shard-indices">TopDocs.merge shall no longer allow setting of shard indices</h3>
<p><code>TopDocs.merge()</code>'s API has been changed to stop allowing passing in a parameter to indicate if it should set shard indices for hits as they are seen during the merge process. This is done to simplify the API to be more dynamic in terms of passing in custom tie breakers. If shard indices are to be used for tie breaking docs with equal scores during <code>TopDocs.merge()</code>, then it is mandatory that the input <code>ScoreDocs</code> have their shard indices set to valid values prior to calling <code>merge()</code></p>
<h3 id="topdocscollector-shall-throw-illegalargumentexception-for-malformed-arguments">TopDocsCollector Shall Throw IllegalArgumentException For Malformed Arguments</h3>
<p><code>TopDocsCollector</code> shall no longer return an empty <code>TopDocs</code> for malformed arguments. Rather, an <code>IllegalArgumentException</code> shall be thrown. This is introduced for better defence and to ensure that there is no bubbling up of errors when Lucene is used in multi level applications</p>
<h3 id="assumption-of-data-consistency-between-different-data-structures-sharing-the-same-field-name">Assumption of data consistency between different data-structures sharing the same field name</h3>
<p>Sorting on a numeric field that is indexed with both doc values and points may use an optimization to skip non-competitive documents. This optimization relies on the assumption that the same data is stored in these points and doc values.</p>
<h3 id="require-consistency-between-data-structures-on-a-per-field-basis">Require consistency between data-structures on a per-field basis</h3>
<p>The per field data-structures are implicitly defined by the first document indexed that contains a certain field. Once defined, the per field data-structures are not changeable for the whole index. For example, if you first index a document where a certain field is indexed with doc values and points, all subsequent documents containing this field must also have this field indexed with only doc values and points.</p>
<p>This also means that an index created in the previous version that doesn't satisfy this requirement can not be updated.</p>
<h3 id="doc-values-updates-are-allowed-only-for-doc-values-only-fields">Doc values updates are allowed only for doc values only fields</h3>
<p>Previously IndexWriter could update doc values for a binary or numeric docValue field that was also indexed with other data structures (e.g. postings, vectors etc). This is not allowed anymore. A field must be indexed with only doc values to be allowed for doc values updates in <code>IndexWriter</code>.</p>
<h3 id="sorteddocvalues-no-longer-extends-binarydocvalues-lucene-9796">SortedDocValues no longer extends BinaryDocValues (<a href="https://issues.apache.org/jira/browse/LUCENE-9796">LUCENE-9796</a>)</h3>
<p><code>SortedDocValues</code> no longer extends <code>BinaryDocValues</code>: <code>SortedDocValues</code> do not have a per-document binary value, they have a per-document numeric <code>ordValue()</code>. The ordinal can then be dereferenced to its binary form with <code>lookupOrd()</code>, but it was a performance trap to implement a <code>binaryValue()</code> on the SortedDocValues api that does this behind-the-scenes on every document.</p>
<p>You can replace calls of <code>binaryValue()</code> with <code>lookupOrd(ordValue())</code> as a &quot;quick fix&quot;, but it is better to use the ordinal alone (integer-based datastructures) for per-document access, and only call <code>lookupOrd()</code> a few times at the end (e.g. for the hits you want to display). Otherwise, if you really don't want per-document ordinals, but instead a per-document <code>byte[]</code>, use a <code>BinaryDocValues</code> field.</p>
<h3 id="removed-codecreaderrambytesused-lucene-9387">Removed CodecReader.ramBytesUsed() (<a href="https://issues.apache.org/jira/browse/LUCENE-9387">LUCENE-9387</a>)</h3>
<p>Lucene index readers are now using so little memory with the default codec that it was decided to remove the ability to estimate their RAM usage.</p>
<h3 id="longvaluefacetcounts-no-longer-accepts-multivalued-param-in-constructors-lucene-9948">LongValueFacetCounts no longer accepts multiValued param in constructors (<a href="https://issues.apache.org/jira/browse/LUCENE-9948">LUCENE-9948</a>)</h3>
<p><code>LongValueFacetCounts</code> will now automatically detect whether-or-not an indexed field is single- or multi-valued. The user no longer needs to provide this information to the ctors. Migrating should be as simple as no longer providing this boolean.</p>
<h3 id="spanquery-and-subclasses-have-moved-from-core-to-the-queries-module">SpanQuery and subclasses have moved from core/ to the queries module</h3>
<p>They can now be found in the <code>org.apache.lucene.queries.spans</code> package.</p>
<h3 id="spanboostquery-has-been-removed-lucene-8143">SpanBoostQuery has been removed (<a href="https://issues.apache.org/jira/browse/LUCENE-8143">LUCENE-8143</a>)</h3>
<p><code>SpanBoostQuery</code> was a no-op unless used at the top level of a <code>SpanQuery</code> nested structure. Use a standard <code>BoostQuery</code> here instead.</p>
<h3 id="sort-is-immutable-lucene-9325">Sort is immutable (<a href="https://issues.apache.org/jira/browse/LUCENE-9325">LUCENE-9325</a>)</h3>
<p>Rather than using <code>setSort()</code> to change sort values, you should instead create a new <code>Sort</code> instance with the new values.</p>
<h3 id="taxonomy-based-faceting-uses-more-modern-encodings-lucene-9450-lucene-10062-lucene-10122">Taxonomy-based faceting uses more modern encodings (<a href="https://issues.apache.org/jira/browse/LUCENE-9450">LUCENE-9450</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-10062">LUCENE-10062</a>, <a href="https://issues.apache.org/jira/browse/LUCENE-10122">LUCENE-10122</a>)</h3>
<p>The side-car taxonomy index now uses doc values for ord-to-path lookup (<a href="https://issues.apache.org/jira/browse/LUCENE-9450">LUCENE-9450</a>) and parent lookup (<a href="https://issues.apache.org/jira/browse/LUCENE-10122">LUCENE-10122</a>) instead of stored fields and positions (respectively). Document ordinals are now encoded with <code>SortedNumericDocValues</code> instead of using a custom (v-int) binary format. Performance gains have been observed with these encoding changes. These changes were introduced in 9.0, and 9.x releases remain backwards-compatible with 8.x indexes, but starting with 10.0, only the newer formats are supported. Users will need to create a new index with all their documents using 9.0 or later to pick up the new format and remain compatible with 10.x releases. Just re-adding documents to an existing index is not enough to pick up the changes as the format will &quot;stick&quot; to whatever version was used to initially create the index.</p>
<p>Additionally, <code>OrdinalsReader</code> (and sub-classes) are fully removed starting with 10.0. These classes were <code>@Deprecated</code> starting with 9.0. Users are encouraged to rely on the default taxonomy facet encodings where possible. If custom formats are needed, users will need to manage the indexed data on their own and create new <code>Facet</code> implementations to use it.</p>
<h3 id="weightscorersupplier-is-declared-abstract-and-weightscorer-methd-is-marked-final"><code>Weight#scorerSupplier</code> is declared abstract, and <code>Weight#scorer</code> methd is marked final</h3>
<p>The <code>Weight#scorerSupplier</code> method is now declared abstract, compelling child classes to implement the ScorerSupplier interface. Additionally, <code>Weight#scorer</code> is now declared final, with its implementation being delegated to <code>Weight#scorerSupplier</code> for the scorer.</p>
<h3 id="reference-to-weight-is-removed-from-scorer-github13410">Reference to <code>weight</code> is removed from Scorer (<a href="https://github.com/apache/lucene/issues/13410">GITHUB#13410</a>)</h3>
<p>The <code>weight</code> has been removed from the Scorer class. Consequently, the constructor, <code>Scorer(Weight)</code>,and a getter, <code>Scorer#getWeight</code>, has also been eliminated. References to weight have also been removed from nearly all the subclasses of Scorer, including ConstantScoreScorer, TermScorer, and others.</p>
<p>Additionally, several APIs have been modified to remove the weight reference, as it is no longer necessary. Specifically, the method <code>FunctionValues#getScorer(Weight weight, LeafReaderContext readerContext)</code> has been updated to <code>FunctionValues#getScorer(LeafReaderContext readerContext)</code>.</p>
<p>Callers must now keep track of the Weight instance that created the Scorer if they need it, instead of relying on Scorer.</p>
<h3 id="facetscollectorsearch-utility-methods-moved-and-updated"><code>FacetsCollector#search</code> utility methods moved and updated</h3>
<p>The static <code>search</code> methods exposed by <code>FacetsCollector</code> have been moved to <code>FacetsCollectorManager</code>. Furthermore, they take a <code>FacetsCollectorManager</code> last argument in place of a <code>Collector</code> so that they support intra query concurrency. The return type has also be updated to <code>FacetsCollectorManager.FacetsResult</code> which includes both <code>TopDocs</code> as well as facets results included in a reduced <code>FacetsCollector</code> instance.</p>
<h3 id="searchwithcollectortask-no-longer-supports-the-collectorclass-config-parameter"><code>SearchWithCollectorTask</code> no longer supports the <code>collector.class</code> config parameter</h3>
<p><code>collector.class</code> used to allow users to load a custom collector implementation. <code>collector.manager.class</code> replaces it by allowing users to load a custom collector manager instead.</p>
<h3 id="bulkscorerscoreleafcollector-collector-bits-acceptdocs-removed">BulkScorer#score(LeafCollector collector, Bits acceptDocs) removed</h3>
<p>Use <code>BulkScorer#score(LeafCollector collector, Bits acceptDocs, int min, int max)</code> instead. In order to score the entire leaf, provide <code>0</code> as min and <code>DocIdSetIterator.NO_MORE_DOCS</code> as max. <code>BulkScorer</code> subclasses that override such method need to instead override the method variant that takes the range of doc ids as well as arguments.</p>
<h3 id="collectormanagernewcollector-and-collectorgetleafcollector-contract">CollectorManager#newCollector and Collector#getLeafCollector contract</h3>
<p>With the introduction of intra-segment query concurrency support, multiple <code>LeafCollector</code>s may be requested for the same <code>LeafReaderContext</code> via <code>Collector#getLeafCollector(LeafReaderContext)</code> across the different <code>Collector</code> instances returned by multiple <code>CollectorManager#newCollector</code> calls. Any logic or computation that needs to happen once per segment requires specific handling in the collector manager implementation. See <code>TotalHitCountCollectorManager</code> as an example. Individual collectors don't need to be adapted as a specific <code>Collector</code> instance will still see a given <code>LeafReaderContext</code> once, given that it is not possible to add more than one partition of the same segment to the same leaf slice.</p>
<h3 id="weightscorer-weightbulkscorer-and-weightscorersupplier-contract">Weight#scorer, Weight#bulkScorer and Weight#scorerSupplier contract</h3>
<p>With the introduction of intra-segment query concurrency support, multiple <code>Scorer</code>s, <code>ScorerSupplier</code>s or <code>BulkScorer</code>s may be requested for the same <code>LeafReaderContext</code> instance as part of a single search call. That may happen concurrently from separate threads each searching a specific doc id range of the segment. <code>Weight</code> implementations that rely on the assumption that a scorer, bulk scorer or scorer supplier for a given <code>LeafReaderContext</code> is requested once per search need updating.</p>
<h3 id="signature-of-indexsearchersearchleaf-changed">Signature of IndexSearcher#searchLeaf changed</h3>
<p>With the introduction of intra-segment query concurrency support, the <code>IndexSearcher#searchLeaf(LeafReaderContext ctx, Weight weight, Collector collector)</code> method now accepts two additional int arguments to identify the min/max range of doc ids that will be searched in this leaf partition<code>: IndexSearcher#searchLeaf(LeafReaderContext ctx, int minDocId, int maxDocId, Weight weight, Collector collector)</code>. Subclasses of <code>IndexSearcher</code> that call or override the <code>searchLeaf</code> method need to be updated accordingly.</p>
<h3 id="signature-of-static-indexsearchslices-method-changed">Signature of static IndexSearch#slices method changed</h3>
<p>The static <code>IndexSearcher#slices(List&lt;LeafReaderContext&gt; leaves, int maxDocsPerSlice, int maxSegmentsPerSlice)</code> method now supports an additional 4th and last argument to optionally enable creating segment partitions: <code>IndexSearcher#slices(List&lt;LeafReaderContext&gt; leaves, int maxDocsPerSlice, int maxSegmentsPerSlice, boolean allowSegmentPartitions)</code></p>
<h3 id="totalhitcountcollectormanager-constructor">TotalHitCountCollectorManager constructor</h3>
<p><code>TotalHitCountCollectorManager</code> now requires that an array of <code>LeafSlice</code>s, retrieved via <code>IndexSearcher#getSlices</code>, is provided to its constructor. Depending on whether segment partitions are present among slices, the manager can optimize the type of collectors it creates and exposes via <code>newCollector</code>.</p>
<h3 id="indexsearchersearchlistleafreadercontext-weight-collector-removed"><code>IndexSearcher#search(List&lt;LeafReaderContext&gt;, Weight, Collector)</code> removed</h3>
<p>The protected <code>IndexSearcher#search(List&lt;LeafReaderContext&gt; leaves, Weight weight, Collector collector)</code> method has been removed in favour of the newly introduced <code>search(LeafReaderContextPartition[] partitions, Weight weight, Collector collector)</code>. <code>IndexSearcher</code> subclasses that override this method need to instead override the new method.</p>
<h3 id="indexing-vectors-with-8-bit-scalar-quantization-is-no-longer-supported-but-7-and-4-bit-quantization-still-work-github13519">Indexing vectors with 8 bit scalar quantization is no longer supported but 7 and 4 bit quantization still work (<a href="https://github.com/apache/lucene/issues/13519">GITHUB#13519</a>)</h3>
<p>8 bit scalar vector quantization is no longer supported: it was buggy starting in 9.11 (<a href="https://github.com/apache/lucene/issues/13197">GITHUB#13197</a>).  4 and 7 bit quantization are still supported.  Existing (9.11) Lucene indices that previously used 8 bit quantization can still be read/searched but the results from <code>KNN*VectorQuery</code> are silently buggy.  Further 8 bit quantized vector indexing into such (9.11) indices is not permitted, so your path forward if you wish to continue using the same 9.11 index is to index additional vectors into the same field with either 4 or 7 bit quantization (or no quantization), and ensure all older (9.x written) segments are rewritten either via <code>IndexWriter.forceMerge</code> or <code>IndexWriter.addIndexes(CodecReader...)</code>, or reindexing entirely.</p>
</body>
</html>
